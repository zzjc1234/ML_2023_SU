{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manifold Learning\n",
    "\n",
    "Manifold learning is an approach to non-linear dimensionality reduction. Algorithms for this task are based on the idea that the dimensionality of many data sets is only artificially high.\n",
    "\n",
    "Though we have both supervised and unsupervised linear dimensionality reduction frameworks such as PCA, Independent Component Analysis, Linear Discriminant Analysis, etc. Some non-linear features will be ignored. Manifold is a try to catch the non-linear feature in high dimension data.\n",
    "\n",
    "Typical manifold learning is unsupervised since the algorithm is learning the structure of the data instead of the pre-defined class.\n",
    "\n",
    "---\n",
    "\n",
    "## Isomap\n",
    "\n",
    "One of the earliest approaches to manifold learning is the Isomap algorithm, short for Isometric Mapping. Isomap can be viewed as an extension of Multi-dimensional Scaling (MDS) or Kernel PCA. Isomap seeks a lower-dimensional embedding which maintains geodesic distances between all points.\n",
    "\n",
    "---\n",
    "\n",
    "## Locally Linear Embedding\n",
    "\n",
    "Locally linear embedding (LLE) seeks a lower-dimensional projection of the data which preserves distances within local neighborhoods. It can be thought of as a series of local Principal Component Analyses which are globally compared to find the best non-linear embedding.\n",
    "\n",
    "One well-known issue with LLE is the regularization problem. When the number of neighbors is greater than the number of input dimensions, the matrix defining each local neighborhood is rank-deficient. To address this, standard LLE applies an arbitrary regularization parameter r, which is chosen relative to the trace of the local weight matrix. Though it can be shown formally that as $r\n",
    "\\rightarrow 0$, the solution converges to the desired embedding, there is no guarantee that the optimal solution will be found for $r > 0$. This problem manifests itself in embeddings which distort the underlying geometry of the manifold.\n",
    "\n",
    "---\n",
    "\n",
    "## Modified Locally Linear Embedding\n",
    "\n",
    "Modified Locally Linear Embedding solves the regularization problem by using multiple weight vectors in each neighborhood. MLLE can be performed with function locally_linear_embedding or its object-oriented counterpart LocallyLinearEmbedding, with the keyword method = 'modified'. It requires n_neighbors > n_components.\n",
    "\n",
    "---\n",
    "\n",
    "## Hessian Eigenmapping\n",
    "\n",
    "Hessian Eigenmapping (also known as Hessian-based LLE: HLLE) is another method of solving the regularization problem of LLE. It revolves around a hessian-based quadratic form at each neighborhood which is used to recover the locally linear structure. Though other implementations note its poor scaling with data size, sklearn implements some algorithmic improvements which make its cost comparable to that of other LLE variants for small output dimension. HLLE can be performed with function locally_linear_embedding or its object-oriented counterpart LocallyLinearEmbedding, with the keyword method = 'hessian'. It requires n_neighbors > n_components * (n_components + 3) / 2.\n",
    "\n",
    "---\n",
    "\n",
    "## Spectral Embedding\n",
    "\n",
    "Spectral Embedding is an approach to calculating a non-linear embedding. Scikit-learn implements Laplacian Eigenmaps, which finds a low dimensional representation of the data using a spectral decomposition of the graph Laplacian. The graph generated can be considered as a discrete approximation of the low dimensional manifold in the high dimensional space. Minimization of a cost function based on the graph ensures that points close to each other on the manifold are mapped close to each other in the low dimensional space, preserving local distances. Spectral embedding can be performed with the function spectral_embedding or its object-oriented counterpart SpectralEmbedding.\n",
    "\n",
    "---\n",
    "\n",
    "## Local Tangent Space Alignment\n",
    "\n",
    "Though not technically a variant of LLE, Local tangent space alignment (LTSA) is algorithmically similar enough to LLE that it can be put in this category. Rather than focusing on preserving neighborhood distances as in LLE, LTSA seeks to characterize the local geometry at each neighborhood via its tangent space, and performs a global optimization to align these local tangent spaces to learn the embedding. LTSA can be performed with function locally_linear_embedding or its object-oriented counterpart LocallyLinearEmbedding, with the keyword method = 'ltsa'.\n",
    "\n",
    "---\n",
    "\n",
    "## Multi-dimensional Scaling (MDS)\n",
    "\n",
    "Multidimensional scaling (MDS) seeks a low-dimensional representation of the data in which the distances respect well the distances in the original high-dimensional space.\n",
    "\n",
    "In general, MDS is a technique used for analyzing similarity or dissimilarity data. It attempts to model similarity or dissimilarity data as distances in a geometric spaces. The data can be ratings of similarity between objects, interaction frequencies of molecules, or trade indices between countries.\n",
    "\n",
    "There exists two types of MDS algorithm: metric and non metric. In scikit-learn, the class MDS implements both. In Metric MDS, the input similarity matrix arises from a metric (and thus respects the triangular inequality), the distances between output two points are then set to be as close as possible to the similarity or dissimilarity data. In the non-metric version, the algorithms will try to preserve the order of the distances, and hence seek for a monotonic relationship between the distances in the embedded space and the similarities/dissimilarities.\n",
    "\n",
    "---\n",
    "\n",
    "## t-distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "Mainly for high dimension data\n",
    "\n",
    "- Advantage\n",
    "  - Revealing the structure at many scales on a single map\n",
    "  - Revealing data that lie in multiple, different, manifolds or clusters\n",
    "  - Reducing the tendency to crowd points together at the center\n",
    "- Disadvantage\n",
    "  - t-SNE is computationally expensive, and can take several hours on million-sample datasets where PCA will finish in seconds or minutes\n",
    "  - The Barnes-Hut t-SNE method is limited to two or three dimensional embeddings.\n",
    "  - The algorithm is stochastic and multiple restarts with different seeds can yield different embeddings. However, it is perfectly legitimate to pick the embedding with the least error.\n",
    "  - Global structure is not explicitly preserved. This problem is mitigated by initializing points with PCA (using init='pca').\n",
    "\n",
    "---\n",
    "\n",
    "## Optimizing t-SNE\n",
    "\n",
    "See user guide\n",
    "\n",
    "---\n",
    "\n",
    "## Barnes-Hut t-SNE\n",
    "\n",
    "- The Barnes-Hut implementation only works when the target dimensionality is 3 or less. The 2D case is typical when building visualizations.\n",
    "- Barnes-Hut only works with dense input data. Sparse data matrices can only be embedded with the exact method or can be approximated by a dense low rank projection for instance using TruncatedSVD\n",
    "- Barnes-Hut is an approximation of the exact method. The approximation is parameterized with the angle parameter, therefore the angle parameter is unused when method=”exact”\n",
    "- Barnes-Hut is significantly more scalable. Barnes-Hut can be used to embed hundred of thousands of data points while the exact method can handle thousands of samples before becoming computationally intractable\n",
    "\n",
    "For visualization purpose (which is the main use case of t-SNE), using the Barnes-Hut method is strongly recommended. The exact t-SNE method is useful for checking the theoretically properties of the embedding possibly in higher dimensional space but limit to small datasets due to computational constraints.\n",
    "\n",
    "Also note that the digits labels roughly match the natural grouping found by t-SNE while the linear 2D projection of the PCA model yields a representation where label regions largely overlap. This is a strong clue that this data can be well separated by non linear methods that focus on the local structure (e.g. an SVM with a Gaussian RBF kernel). However, failing to visualize well separated homogeneously labeled groups with t-SNE in 2D does not necessarily imply that the data cannot be correctly classified by a supervised model. It might be the case that 2 dimensions are not high enough to accurately represent the internal structure of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Manifold Learning methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# unused but required import for doing 3d projections with matplotlib < 3.2\n",
    "import mpl_toolkits.mplot3d  # noqa: F401\n",
    "from matplotlib import ticker\n",
    "\n",
    "from sklearn import datasets, manifold\n",
    "\n",
    "n_samples = 1500\n",
    "S_points, S_color = datasets.make_s_curve(n_samples, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d(points, points_color, title):\n",
    "    x, y, z = points.T\n",
    "\n",
    "    fig, ax = plt.subplots(\n",
    "        figsize=(6, 6),\n",
    "        facecolor=\"white\",\n",
    "        tight_layout=True,\n",
    "        subplot_kw={\"projection\": \"3d\"},\n",
    "    )\n",
    "    fig.suptitle(title, size=16)\n",
    "    col = ax.scatter(x, y, z, c=points_color, s=50, alpha=0.8)\n",
    "    ax.view_init(azim=-60, elev=9)\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.zaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    fig.colorbar(col, ax=ax, orientation=\"horizontal\", shrink=0.6, aspect=60, pad=0.01)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_2d(points, points_color, title):\n",
    "    fig, ax = plt.subplots(figsize=(3, 3), facecolor=\"white\", constrained_layout=True)\n",
    "    fig.suptitle(title, size=16)\n",
    "    add_2d_scatter(ax, points, points_color)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def add_2d_scatter(ax, points, points_color, title=None):\n",
    "    x, y = points.T\n",
    "    ax.scatter(x, y, c=points_color, s=50, alpha=0.8)\n",
    "    ax.set_title(title)\n",
    "    ax.xaxis.set_major_formatter(ticker.NullFormatter())\n",
    "    ax.yaxis.set_major_formatter(ticker.NullFormatter())\n",
    "\n",
    "\n",
    "plot_3d(S_points, S_color, \"Original S-curve samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 12  # neighborhood which is used to recover the locally linear structure\n",
    "n_components = 2  # number of coordinates for the manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"n_neighbors\": n_neighbors,\n",
    "    \"n_components\": n_components,\n",
    "    \"eigen_solver\": \"auto\",\n",
    "    \"random_state\": 0,\n",
    "}\n",
    "\n",
    "lle_standard = manifold.LocallyLinearEmbedding(method=\"standard\", **params)\n",
    "S_standard = lle_standard.fit_transform(S_points)\n",
    "\n",
    "lle_ltsa = manifold.LocallyLinearEmbedding(method=\"ltsa\", **params)\n",
    "S_ltsa = lle_ltsa.fit_transform(S_points)\n",
    "\n",
    "lle_hessian = manifold.LocallyLinearEmbedding(method=\"hessian\", **params)\n",
    "S_hessian = lle_hessian.fit_transform(S_points)\n",
    "\n",
    "lle_mod = manifold.LocallyLinearEmbedding(method=\"modified\", **params)\n",
    "S_mod = lle_mod.fit_transform(S_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(\n",
    "    nrows=2, ncols=2, figsize=(7, 7), facecolor=\"white\", constrained_layout=True\n",
    ")\n",
    "fig.suptitle(\"Locally Linear Embeddings\", size=16)\n",
    "\n",
    "lle_methods = [\n",
    "    (\"Standard locally linear embedding\", S_standard),\n",
    "    (\"Local tangent space alignment\", S_ltsa),\n",
    "    (\"Hessian eigenmap\", S_hessian),\n",
    "    (\"Modified locally linear embedding\", S_mod),\n",
    "]\n",
    "for ax, method in zip(axs.flat, lle_methods):\n",
    "    name, points = method\n",
    "    add_2d_scatter(ax, points, S_color, name)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isomap = manifold.Isomap(n_neighbors=n_neighbors, n_components=n_components, p=1)\n",
    "S_isomap = isomap.fit_transform(S_points)\n",
    "\n",
    "plot_2d(S_isomap, S_color, \"Isomap Embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_scaling = manifold.MDS(\n",
    "    n_components=n_components,\n",
    "    max_iter=50,\n",
    "    n_init=4,\n",
    "    random_state=0,\n",
    "    normalized_stress=False,\n",
    ")\n",
    "S_scaling = md_scaling.fit_transform(S_points)\n",
    "\n",
    "plot_2d(S_scaling, S_color, \"Multidimensional scaling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral = manifold.SpectralEmbedding(\n",
    "    n_components=n_components, n_neighbors=n_neighbors\n",
    ")\n",
    "S_spectral = spectral.fit_transform(S_points)\n",
    "\n",
    "plot_2d(S_spectral, S_color, \"Spectral Embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sne = manifold.TSNE(\n",
    "    n_components=n_components,\n",
    "    perplexity=30,\n",
    "    init=\"random\",\n",
    "    n_iter=250,\n",
    "    random_state=0,\n",
    ")\n",
    "S_t_sne = t_sne.fit_transform(S_points)\n",
    "\n",
    "plot_2d(S_t_sne, S_color, \"T-distributed Stochastic  \\n Neighbor Embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manifold learning on handwritten digits: Locally Linear Embedding, Isomap…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits(n_class=6)\n",
    "X, y = digits.data, digits.target\n",
    "n_samples, n_features = X.shape\n",
    "n_neighbors = 30\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(nrows=10, ncols=10, figsize=(6, 6))\n",
    "for idx, ax in enumerate(axs.ravel()):\n",
    "    ax.imshow(X[idx].reshape((8, 8)), cmap=plt.cm.binary)\n",
    "    ax.axis(\"off\")\n",
    "_ = fig.suptitle(\"A selection from the 64-dimensional digits dataset\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import offsetbox\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "def plot_embedding(X, title):\n",
    "    _, ax = plt.subplots()\n",
    "    X = MinMaxScaler().fit_transform(X)\n",
    "\n",
    "    for digit in digits.target_names:\n",
    "        ax.scatter(\n",
    "            *X[y == digit].T,\n",
    "            marker=f\"${digit}$\",\n",
    "            s=60,\n",
    "            color=plt.cm.Dark2(digit),\n",
    "            alpha=0.425,\n",
    "            zorder=2,\n",
    "        )\n",
    "    shown_images = np.array([[1.0, 1.0]])  # just something big\n",
    "    for i in range(X.shape[0]):\n",
    "        # plot every digit on the embedding\n",
    "        # show an annotation box for a group of digits\n",
    "        dist = np.sum((X[i] - shown_images) ** 2, 1)\n",
    "        if np.min(dist) < 4e-3:\n",
    "            # don't show points that are too close\n",
    "            continue\n",
    "        shown_images = np.concatenate([shown_images, [X[i]]], axis=0)\n",
    "        imagebox = offsetbox.AnnotationBbox(\n",
    "            offsetbox.OffsetImage(digits.images[i], cmap=plt.cm.gray_r), X[i]\n",
    "        )\n",
    "        imagebox.set(zorder=1)\n",
    "        ax.add_artist(imagebox)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomTreesEmbedding\n",
    "from sklearn.manifold import (\n",
    "    MDS,\n",
    "    TSNE,\n",
    "    Isomap,\n",
    "    LocallyLinearEmbedding,\n",
    "    SpectralEmbedding,\n",
    ")\n",
    "from sklearn.neighbors import NeighborhoodComponentsAnalysis\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "embeddings = {\n",
    "    \"Random projection embedding\": SparseRandomProjection(\n",
    "        n_components=2, random_state=42\n",
    "    ),\n",
    "    \"Truncated SVD embedding\": TruncatedSVD(n_components=2),\n",
    "    \"Linear Discriminant Analysis embedding\": LinearDiscriminantAnalysis(\n",
    "        n_components=2\n",
    "    ),\n",
    "    \"Isomap embedding\": Isomap(n_neighbors=n_neighbors, n_components=2),\n",
    "    \"Standard LLE embedding\": LocallyLinearEmbedding(\n",
    "        n_neighbors=n_neighbors, n_components=2, method=\"standard\"\n",
    "    ),\n",
    "    \"Modified LLE embedding\": LocallyLinearEmbedding(\n",
    "        n_neighbors=n_neighbors, n_components=2, method=\"modified\"\n",
    "    ),\n",
    "    \"Hessian LLE embedding\": LocallyLinearEmbedding(\n",
    "        n_neighbors=n_neighbors, n_components=2, method=\"hessian\"\n",
    "    ),\n",
    "    \"LTSA LLE embedding\": LocallyLinearEmbedding(\n",
    "        n_neighbors=n_neighbors, n_components=2, method=\"ltsa\"\n",
    "    ),\n",
    "    \"MDS embedding\": MDS(\n",
    "        n_components=2, n_init=1, max_iter=120, n_jobs=2, normalized_stress=\"auto\"\n",
    "    ),\n",
    "    \"Random Trees embedding\": make_pipeline(\n",
    "        RandomTreesEmbedding(n_estimators=200, max_depth=5, random_state=0),\n",
    "        TruncatedSVD(n_components=2),\n",
    "    ),\n",
    "    \"Spectral embedding\": SpectralEmbedding(\n",
    "        n_components=2, random_state=0, eigen_solver=\"arpack\"\n",
    "    ),\n",
    "    \"t-SNE embeedding\": TSNE(\n",
    "        n_components=2,\n",
    "        n_iter=500,\n",
    "        n_iter_without_progress=150,\n",
    "        n_jobs=2,\n",
    "        random_state=0,\n",
    "    ),\n",
    "    \"NCA embedding\": NeighborhoodComponentsAnalysis(\n",
    "        n_components=2, init=\"pca\", random_state=0\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "projections, timing = {}, {}\n",
    "for name, transformer in embeddings.items():\n",
    "    if name.startswith(\"Linear Discriminant Analysis\"):\n",
    "        data = X.copy()\n",
    "        data.flat[:: X.shape[1] + 1] += 0.01  # Make X invertible\n",
    "    else:\n",
    "        data = X\n",
    "\n",
    "    print(f\"Computing {name}...\")\n",
    "    start_time = time()\n",
    "    projections[name] = transformer.fit_transform(data, y)\n",
    "    timing[name] = time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in timing:\n",
    "    title = f\"{name} (time {timing[name]:.3f}s)\"\n",
    "    plot_embedding(projections[name], title)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
