{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start to talk about the SGD, first we need to have a rough idea about Gradient Descent. Here is a sample code for Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized parameters: [[-4.12341880e+75]\n",
      " [-3.88876855e+75]\n",
      " [-4.76192661e+75]\n",
      " [-3.87814045e+75]\n",
      " [-4.31970070e+75]\n",
      " [-3.81708271e+75]\n",
      " [-4.50407650e+75]\n",
      " [-3.94387939e+75]\n",
      " [-4.28411920e+75]\n",
      " [-4.33983404e+75]\n",
      " [-3.97085642e+75]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# generate data\n",
    "np.random.seed(42)\n",
    "X = 2 * np.random.rand(100, 10)\n",
    "y = 4 + 3 * X.dot(np.random.randn(10, 1)) + np.random.randn(100, 1)\n",
    "\n",
    "# init\n",
    "theta = np.random.randn(11, 1)\n",
    "\n",
    "# adding intercept\n",
    "X_b = np.c_[np.ones((100, 1)), X]\n",
    "\n",
    "learning_rate = 0.1\n",
    "num_iterations = 1000\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "\n",
    "    y_pred = X_b.dot(theta)\n",
    "    \n",
    "    # calculate loss\n",
    "    loss = np.mean((y_pred - y)**2)\n",
    "    \n",
    "    # calculate gradient\n",
    "    gradient = 2 * X_b.T.dot(y_pred - y) / len(y)\n",
    "    \n",
    "    # update parameter\n",
    "    theta = theta - learning_rate * gradient\n",
    "\n",
    "# 输出最终优化的模型参数\n",
    "print(\"Optimized parameters:\", theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the calculation is related to all 10 features. When the number of features go up, the speed of calculation will be incredibly slow. Thus, SGD is created to solve these kind of problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Idea\n",
    "\n",
    "The basic idea of SGD is randomly choosing one feature for each iteration and use the gradient of that feature to substitute the gradient of all features. This means that the calculation speed will be n times faster for n features.\n",
    "\n",
    "## Math Proof\n",
    "\n",
    "Instead of providing strict math proof, I will only provide some qualitative analysis limited by my poor math ability.\n",
    "\n",
    "Here is a illustration of gradient descent algorithm with different iteration step. As we can see, when the iteration step is small, the optimization goes nice and smooth. However, when the iteration step becomes large, the curve goes wildly.\n",
    "\n",
    "![gds](../../../src/gd_s.png)\n",
    "\n",
    "![gdl](../../../src/gd_l.png)\n",
    "\n",
    "For SGD algorithm, the model is far more sensitive to the iteration step. When iteration step becomes large, the fluctuation will become much bigger.\n",
    "\n",
    "![sgds](../../../src/sgd_s.png)\n",
    "\n",
    "![sgdl](../../../src/sgd_l.png)\n",
    "\n",
    "One of important pattern of SGD is that the curve is smooth at beginning which is similar to the gradient descent. However, the fluctuation will appear when curve is close to the optimal target. This pattern will help us prevent overfit and make the mdoel more robust to the noise in the perspective of machine learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
