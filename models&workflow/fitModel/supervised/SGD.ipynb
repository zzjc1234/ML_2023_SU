{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start to talk about the SGD, first we need to have a rough idea about Gradient Descent. Here is a sample code for Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized parameters: [[-4.12341880e+75]\n",
      " [-3.88876855e+75]\n",
      " [-4.76192661e+75]\n",
      " [-3.87814045e+75]\n",
      " [-4.31970070e+75]\n",
      " [-3.81708271e+75]\n",
      " [-4.50407650e+75]\n",
      " [-3.94387939e+75]\n",
      " [-4.28411920e+75]\n",
      " [-4.33983404e+75]\n",
      " [-3.97085642e+75]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# generate data\n",
    "np.random.seed(42)\n",
    "X = 2 * np.random.rand(100, 10)\n",
    "y = 4 + 3 * X.dot(np.random.randn(10, 1)) + np.random.randn(100, 1)\n",
    "\n",
    "# init\n",
    "theta = np.random.randn(11, 1)\n",
    "\n",
    "# adding intercept\n",
    "X_b = np.c_[np.ones((100, 1)), X]\n",
    "\n",
    "learning_rate = 0.1\n",
    "num_iterations = 1000\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "\n",
    "    y_pred = X_b.dot(theta)\n",
    "    \n",
    "    # calculate loss\n",
    "    loss = np.mean((y_pred - y)**2)\n",
    "    \n",
    "    # calculate gradient\n",
    "    gradient = 2 * X_b.T.dot(y_pred - y) / len(y)\n",
    "    \n",
    "    # update parameter\n",
    "    theta = theta - learning_rate * gradient\n",
    "\n",
    "# 输出最终优化的模型参数\n",
    "print(\"Optimized parameters:\", theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the calculation is related to all 10 features. When the number of features go up, the speed of calculation will be incredibly slow. Thus, SGD is created to solve these kind of problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Idea\n",
    "\n",
    "The basic idea of SGD is randomly choosing one feature for each iteration and use the gradient of that feature to substitute the gradient of all features. This means that the calculation speed will be n times faster for n features.\n",
    "\n",
    "## Math Proof\n",
    "\n",
    "Instead of providing strict math proof, I will only provide some qualitative analysis limited by my poor math ability.\n",
    "\n",
    "Here is a illustration of gradient descent algorithm with different iteration step. As we can see, when the iteration step is small, the optimization goes nice and smooth. However, when the iteration step becomes large, the curve goes wildly.\n",
    "\n",
    "![gds](../../../src/gd_s.png)\n",
    "\n",
    "![gdl](../../../src/gd_l.png)\n",
    "\n",
    "For SGD algorithm, the model is far more sensitive to the iteration step. When iteration step becomes large, the fluctuation will become much bigger.\n",
    "\n",
    "![sgds](../../../src/sgd_s.png)\n",
    "\n",
    "![sgdl](../../../src/sgd_l.png)\n",
    "\n",
    "One of important pattern of SGD is that the curve is smooth at beginning which is similar to the gradient descent. However, the fluctuation will appear when curve is close to the optimal target. This pattern will help us prevent overfit and make the mdoel more robust to the noise in the perspective of machine learning. The reason for the confusion can be illustrated by dropping a graph. Suppose we take least square as the loss function, the minimum of one feature is reached when it is equal to $\\frac{a_i}{b_i}$. Thus we get the figure.\n",
    "\n",
    "![fluctuation](../../../src/fluct_reason.jpg)\n",
    "\n",
    "The final solution should lay in the maximum and minimum of these numbers (this can be proved by math). Therefore, when the curve is far from the target, it will go smooth, however, when it approach the target, fluctuation will happen.\n",
    "\n",
    "Moreover, the SGD algorithm is an unbiased estimation of the true gradient.\n",
    "\n",
    "A similar way to improve the gradient descent problem is the mini-batch strategy, which divide the data set into several small batch which balances the SGD and gradient descent.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantage and Disadvantage\n",
    "\n",
    "- Advantage\n",
    "    - Efficiency.\n",
    "    - Ease of implementation (lots of opportunities for code tuning).\n",
    "- Disadvantage\n",
    "  - SGD requires a number of hyperparameters such as the regularization parameter and the number of iterations.\n",
    "  - SGD is sensitive to feature scaling.\n",
    "\n",
    "### Warning and Reason\n",
    "\n",
    "Make sure you permute (shuffle) your training data before fitting the model or use shuffle=True to shuffle after each iteration (used by default). Also, ideally, features should be standardized using e.g. make_pipeline(StandardScaler(), SGDClassifier())\n",
    "\n",
    "The reason for us to shuffle the data everytime after each iteration is that different from the mathematical proof, see in [user guide](https://scikit-learn.org/stable/modules/sgd.html), in the actual world the data are highly possible from different distribution and thus we have to shuffle the data to solve this problem. (for more detailed explanation, you should search for IID probability problem, which is still not solved completely nowadays)\n",
    "\n",
    "## First SGD Model\n",
    "\n",
    "Here is a simple illustration for implementing the SGD with sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "X = [[0., 0.], [1., 1.]]\n",
    "y = [0, 1]\n",
    "clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=5)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict new values\n",
    "clf.predict([[2., 2.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the coefficient, intercept and descision rule of SGD with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.coef_)\n",
    "print(clf.intercept_)\n",
    "print(clf.decision_function([[2., 2.]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method, Parameter and Attribute\n",
    "\n",
    "- Method\n",
    "  - fit()\n",
    "  - predict()\n",
    "- Parameter\n",
    "  - loss\n",
    "\n",
    "    This parameter can assign the loss function of SGD, for detailed information, see [here](https://scikit-learn.org/stable/modules/sgd.html#mathematical-formulation)\n",
    "\n",
    "    - hinge\n",
    "    - modified_huber\n",
    "    - log_loss\n",
    "    - all regression losses\n",
    "  - penalty\n",
    "    - l2\n",
    "    - l1\n",
    "    - elasticnet\n",
    "\n",
    "    Also see [here](https://scikit-learn.org/stable/modules/sgd.html#mathematical-formulation)\n",
    "\n",
    "  - max_iter\n",
    "\n",
    "    Empirically, SGD converges after observing approximately 10^6 training samples. Thus, a reasonable first guess for the number of iterations is max_iter = np.ceil(10**6 / n), where n is the size of the training set.\n",
    "\n",
    "  - predict_proba\n",
    "  - early_stopping\n",
    "    \n",
    "    With early_stopping=True, the input data is split into a training set and a validation set. The model is then fitted on the training set, and the stopping criterion is based on the prediction score (using the score method) computed on the validation set. The size of the validation set can be changed with the parameter validation_fraction.\n",
    "\n",
    "    With early_stopping=False, the model is fitted on the entire input data and the stopping criterion is based on the objective function computed on the training data.\n",
    "\n",
    "    In both cases, the criterion is evaluated once by epoch, and the algorithm stops when the criterion does not improve n_iter_no_change times in a row. The improvement is evaluated with absolute tolerance tol, and the algorithm stops in any case after a maximum number of iteration max_iter.\n",
    "\n",
    "- Attribute\n",
    "  - coef_\n",
    "  - intercept_\n",
    "\n",
    "## Tips\n",
    "\n",
    "Stochastic Gradient Descent is sensitive to feature scaling, so it is highly recommended to scale your data. For example, scale each attribute on the input vector X to [0,1] or [-1,+1], or standardize it to have mean 0 and variance 1. Note that the same scaling must be applied to the test vector to obtain meaningful results. This can be easily done using StandardScaler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)  # Don't cheat - fit only on training data\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)  # apply same transformation to test data\n",
    "\n",
    "# Or better yet: use a pipeline!\n",
    "from sklearn.pipeline import make_pipeline\n",
    "est = make_pipeline(StandardScaler(), SGDClassifier())\n",
    "est.fit(X_train)\n",
    "est.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your attributes have an intrinsic scale (e.g. word frequencies or indicator features) scaling is not needed.\n",
    "\n",
    "Finding a reasonable regularization term $\\alpha$ is best done using automatic hyper-parameter search, e.g. GridSearchCV or RandomizedSearchCV, usually in the range 10.0**-np.arange(1,7)\n",
    "\n",
    "If you apply SGD to features extracted using PCA we found that it is often wise to scale the feature values by some constant c such that the average L2 norm of the training data equals one.\n",
    "\n",
    "Averaged SGD works best with a larger number of features and a higher eta0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Code from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the maximum margin separating hyperplane within a two-class separable dataset using a linear Support Vector Machines classifier trained using SGD.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# we create 50 separable points\n",
    "X, Y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.60)\n",
    "\n",
    "# fit the model\n",
    "clf = SGDClassifier(loss=\"hinge\", alpha=0.01, max_iter=200)\n",
    "\n",
    "clf.fit(X, Y)\n",
    "\n",
    "# plot the line, the points, and the nearest vectors to the plane\n",
    "xx = np.linspace(-1, 5, 10)\n",
    "yy = np.linspace(-1, 5, 10)\n",
    "\n",
    "X1, X2 = np.meshgrid(xx, yy)\n",
    "Z = np.empty(X1.shape)\n",
    "for (i, j), val in np.ndenumerate(X1):\n",
    "    x1 = val\n",
    "    x2 = X2[i, j]\n",
    "    p = clf.decision_function([[x1, x2]])\n",
    "    Z[i, j] = p[0]\n",
    "levels = [-1.0, 0.0, 1.0]\n",
    "linestyles = [\"dashed\", \"solid\", \"dashed\"]\n",
    "colors = \"k\"\n",
    "plt.contour(X1, X2, Z, levels, colors=colors, linestyles=linestyles)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired, edgecolor=\"black\", s=20)\n",
    "\n",
    "plt.axis(\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decision surface of multi-class SGD on iris dataset. The hyperplanes corresponding to the three one-versus-all (OVA) classifiers are represented by the dashed lines.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# we only take the first two features. We could\n",
    "# avoid this ugly slicing by using a two-dim dataset\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "colors = \"bry\"\n",
    "\n",
    "# shuffle\n",
    "idx = np.arange(X.shape[0])\n",
    "np.random.seed(13)\n",
    "np.random.shuffle(idx)\n",
    "X = X[idx]\n",
    "y = y[idx]\n",
    "\n",
    "# standardize\n",
    "mean = X.mean(axis=0)\n",
    "std = X.std(axis=0)\n",
    "X = (X - mean) / std\n",
    "\n",
    "clf = SGDClassifier(alpha=0.001, max_iter=100).fit(X, y)\n",
    "ax = plt.gca()\n",
    "DecisionBoundaryDisplay.from_estimator(\n",
    "    clf,\n",
    "    X,\n",
    "    cmap=plt.cm.Paired,\n",
    "    ax=ax,\n",
    "    response_method=\"predict\",\n",
    "    xlabel=iris.feature_names[0],\n",
    "    ylabel=iris.feature_names[1],\n",
    ")\n",
    "plt.axis(\"tight\")\n",
    "\n",
    "# Plot also the training points\n",
    "for i, color in zip(clf.classes_, colors):\n",
    "    idx = np.where(y == i)\n",
    "    plt.scatter(\n",
    "        X[idx, 0],\n",
    "        X[idx, 1],\n",
    "        c=color,\n",
    "        label=iris.target_names[i],\n",
    "        cmap=plt.cm.Paired,\n",
    "        edgecolor=\"black\",\n",
    "        s=20,\n",
    "    )\n",
    "plt.title(\"Decision surface of multi-class SGD\")\n",
    "plt.axis(\"tight\")\n",
    "\n",
    "# Plot the three one-against-all classifiers\n",
    "xmin, xmax = plt.xlim()\n",
    "ymin, ymax = plt.ylim()\n",
    "coef = clf.coef_\n",
    "intercept = clf.intercept_\n",
    "\n",
    "\n",
    "def plot_hyperplane(c, color):\n",
    "    def line(x0):\n",
    "        return (-(x0 * coef[c, 0]) - intercept[c]) / coef[c, 1]\n",
    "\n",
    "    plt.plot([xmin, xmax], [line(xmin), line(xmax)], ls=\"--\", color=color)\n",
    "\n",
    "\n",
    "for i, color in zip(clf.classes_, colors):\n",
    "    plot_hyperplane(i, color)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decision function of a weighted dataset, where the size of points is proportional to its weight.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "# we create 20 points\n",
    "np.random.seed(0)\n",
    "X = np.r_[np.random.randn(10, 2) + [1, 1], np.random.randn(10, 2)]\n",
    "y = [1] * 10 + [-1] * 10\n",
    "sample_weight = 100 * np.abs(np.random.randn(20))\n",
    "# and assign a bigger weight to the last 10 samples\n",
    "sample_weight[:10] *= 10\n",
    "\n",
    "# plot the weighted data points\n",
    "xx, yy = np.meshgrid(np.linspace(-4, 5, 500), np.linspace(-4, 5, 500))\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(\n",
    "    X[:, 0],\n",
    "    X[:, 1],\n",
    "    c=y,\n",
    "    s=sample_weight,\n",
    "    alpha=0.9,\n",
    "    cmap=plt.cm.bone,\n",
    "    edgecolor=\"black\",\n",
    ")\n",
    "\n",
    "# fit the unweighted model\n",
    "clf = linear_model.SGDClassifier(alpha=0.01, max_iter=100)\n",
    "clf.fit(X, y)\n",
    "Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "no_weights = ax.contour(xx, yy, Z, levels=[0], linestyles=[\"solid\"])\n",
    "\n",
    "# fit the weighted model\n",
    "clf = linear_model.SGDClassifier(alpha=0.01, max_iter=100)\n",
    "clf.fit(X, y, sample_weight=sample_weight)\n",
    "Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "samples_weights = ax.contour(xx, yy, Z, levels=[0], linestyles=[\"dashed\"])\n",
    "\n",
    "no_weights_handles, _ = no_weights.legend_elements()\n",
    "weights_handles, _ = samples_weights.legend_elements()\n",
    "ax.legend(\n",
    "    [no_weights_handles[0], weights_handles[0]],\n",
    "    [\"no weights\", \"with weights\"],\n",
    "    loc=\"lower left\",\n",
    ")\n",
    "\n",
    "ax.set(xticks=(), yticks=())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example showing how different online solvers perform on the hand-written digits dataset.\n",
    "# Author: Rob Zinkov <rob at zinkov dot com>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import (\n",
    "    LogisticRegression,\n",
    "    PassiveAggressiveClassifier,\n",
    "    Perceptron,\n",
    "    SGDClassifier,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "heldout = [0.95, 0.90, 0.75, 0.50, 0.01]\n",
    "# Number of rounds to fit and evaluate an estimator.\n",
    "rounds = 10\n",
    "X, y = datasets.load_digits(return_X_y=True)\n",
    "\n",
    "classifiers = [\n",
    "    (\"SGD\", SGDClassifier(max_iter=110)),\n",
    "    (\"ASGD\", SGDClassifier(max_iter=110, average=True)),\n",
    "    (\"Perceptron\", Perceptron(max_iter=110)),\n",
    "    (\n",
    "        \"Passive-Aggressive I\",\n",
    "        PassiveAggressiveClassifier(max_iter=110, loss=\"hinge\", C=1.0, tol=1e-4),\n",
    "    ),\n",
    "    (\n",
    "        \"Passive-Aggressive II\",\n",
    "        PassiveAggressiveClassifier(\n",
    "            max_iter=110, loss=\"squared_hinge\", C=1.0, tol=1e-4\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"SAG\",\n",
    "        LogisticRegression(max_iter=110, solver=\"sag\", tol=1e-1, C=1.0e4 / X.shape[0]),\n",
    "    ),\n",
    "]\n",
    "\n",
    "xx = 1.0 - np.array(heldout)\n",
    "\n",
    "for name, clf in classifiers:\n",
    "    print(\"training %s\" % name)\n",
    "    rng = np.random.RandomState(42)\n",
    "    yy = []\n",
    "    for i in heldout:\n",
    "        yy_ = []\n",
    "        for r in range(rounds):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=i, random_state=rng\n",
    "            )\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "            yy_.append(1 - np.mean(y_pred == y_test))\n",
    "        yy.append(np.mean(yy_))\n",
    "    plt.plot(xx, yy, label=name)\n",
    "\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"Proportion train\")\n",
    "plt.ylabel(\"Test Error Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the optimal separating hyperplane using an SVC for classes that are unbalanced.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "# we create two clusters of random points\n",
    "n_samples_1 = 1000\n",
    "n_samples_2 = 100\n",
    "centers = [[0.0, 0.0], [2.0, 2.0]]\n",
    "clusters_std = [1.5, 0.5]\n",
    "X, y = make_blobs(\n",
    "    n_samples=[n_samples_1, n_samples_2],\n",
    "    centers=centers,\n",
    "    cluster_std=clusters_std,\n",
    "    random_state=0,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# fit the model and get the separating hyperplane\n",
    "clf = svm.SVC(kernel=\"linear\", C=1.0)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# fit the model and get the separating hyperplane using weighted classes\n",
    "wclf = svm.SVC(kernel=\"linear\", class_weight={1: 10})\n",
    "wclf.fit(X, y)\n",
    "\n",
    "# plot the samples\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, edgecolors=\"k\")\n",
    "\n",
    "# plot the decision functions for both classifiers\n",
    "ax = plt.gca()\n",
    "disp = DecisionBoundaryDisplay.from_estimator(\n",
    "    clf,\n",
    "    X,\n",
    "    plot_method=\"contour\",\n",
    "    colors=\"k\",\n",
    "    levels=[0],\n",
    "    alpha=0.5,\n",
    "    linestyles=[\"-\"],\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "# plot decision boundary and margins for weighted classes\n",
    "wdisp = DecisionBoundaryDisplay.from_estimator(\n",
    "    wclf,\n",
    "    X,\n",
    "    plot_method=\"contour\",\n",
    "    colors=\"r\",\n",
    "    levels=[0],\n",
    "    alpha=0.5,\n",
    "    linestyles=[\"-\"],\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "plt.legend(\n",
    "    [disp.surface_.collections[0], wdisp.surface_.collections[0]],\n",
    "    [\"non weighted\", \"weighted\"],\n",
    "    loc=\"upper right\",\n",
    ")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
