{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbour\n",
    "\n",
    "Nearest neightbour provides functionality for unsupervised and supervised neighbors-based learning methods. Unsupervised nearest neighbors is the foundation of many other learning methods, notably manifold learning and spectral clustering. Supervised neighbors-based learning comes in two flavors: classification for data with discrete labels, and regression for data with continuous labels.\n",
    "\n",
    "The principle behind nearest neighbor methods is to find a predefined number of training samples closest in distance to the new point, and predict the label from these. The number of samples can be a user-defined constant (k-nearest neighbor learning), or vary based on the local density of points (radius-based neighbor learning). The distance can, in general, be any metric measure: standard Euclidean distance is the most common choice.\n",
    "\n",
    "Being a non-parametric method, it is often successful in classification situations where the decision boundary is very irregular.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math Principle\n",
    "\n",
    "The classification is based on the class of the nearest neighbours. I.e. the point will check the most representative nearest neighbour's class and decide the class.\n",
    "\n",
    "A very commonly used algorithm is KNN, in which k stands for the number of neighbor the point will check.\n",
    "\n",
    "The sklearn also provide rnn, where the algorithm will check all points's classes within the radius assigned by the user.\n",
    "\n",
    "For knn, which is more commonly used, the k value is highly dependent to the data. Large the k value, more robust the model, but less clear the boundries. This issue can be solved with Silhouette Score and other evaluators.\n",
    "\n",
    "For rnn, it is a better choice when data is not sampled uniformly. But for higher dimension data, its performance is poor.\n",
    "\n",
    "By default, the data's weight is the same, user can change the weight with parameter `weights`. \n",
    "\n",
    "---\n",
    "\n",
    "## Realization\n",
    "\n",
    "1. Brute Force\n",
    "   \n",
    "   This algorithm is useful for small data. We can use it by setting parameter `algorithm=brute`\n",
    "\n",
    "2. K-D Tree\n",
    "   \n",
    "   K-D Tree is a more efficient algorithm comparing to the brute force. The basic idea is to generate a k dimensional $2^k$ tree recursively partitions the parameter space along the data axes, dividing it into nested orthotropic regions into which data points are filed.\n",
    "\n",
    "   Once the K-D Tree is constructed, it will be super fast to check the nearest neighbor. However, as the dimension goes up, the speed of checking nearest points become slow.\n",
    "\n",
    "3. Ball Tree\n",
    "   \n",
    "   To address the inefficiencies of KD Trees in higher dimensions, the ball tree data structure was developed. Where KD trees partition data along Cartesian axes, ball trees partition data in a series of nesting hyper-spheres. This makes tree construction more costly than that of the KD tree, but results in a data structure which can be very efficient on highly structured data, even in very high dimensions.\n",
    "\n",
    "   A ball tree recursively divides the data into nodes defined by a centroid $C$ and radius $r$ , such that each point in the node lies within the hyper-sphere defined by $r$ and $C$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Code for KDTree\n",
    "# Just a demo, plz don't write it yourself\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, point, left=None, right=None):\n",
    "        self.point = point\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "def build_kd_tree(points, depth=0):\n",
    "    if not points:\n",
    "        return None\n",
    "\n",
    "    k = len(points[0])\n",
    "    axis = depth % k\n",
    "    points.sort(key=lambda point: point[axis])\n",
    "    median_idx = len(points) // 2\n",
    "    median = points[median_idx]\n",
    "    \n",
    "    left_points = points[:median_idx]\n",
    "    right_points = points[median_idx + 1:]\n",
    "    \n",
    "    left_subtree = build_kd_tree(left_points, depth + 1)\n",
    "    right_subtree = build_kd_tree(right_points, depth + 1)\n",
    "    \n",
    "    return Node(median, left_subtree, right_subtree)\n",
    "\n",
    "def print_tree(node, level=0):\n",
    "    if node:\n",
    "        print(level, node.point)\n",
    "        print_tree(node.left, level + 1)\n",
    "        print_tree(node.right, level + 1)\n",
    "\n",
    "data = [(2,3,4), (5,4,6), (9,6,8), (4,7,2), (8,1,5), (7,2,3)]\n",
    "root = build_kd_tree(data)\n",
    "\n",
    "print_tree(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Code for Ball Tree\n",
    "# Just a demo, plz don't write it yourself\n",
    "import numpy as np\n",
    "\n",
    "class BallNode:\n",
    "    def __init__(self, center, radius, left=None, right=None, points=None):\n",
    "        self.center = center  # Ball 的球心\n",
    "        self.radius = radius  # Ball 的半径\n",
    "        self.left = left  # 左子树\n",
    "        self.right = right  # 右子树\n",
    "        self.points = points  # 子树中的数据点\n",
    "\n",
    "def build_ball_tree(points):\n",
    "    if len(points) == 0:\n",
    "        return None\n",
    "\n",
    "    center = np.mean(points, axis=0)  # 计算球心\n",
    "    radius = np.max(np.linalg.norm(points - center, axis=1))  # 计算半径\n",
    "\n",
    "    if len(points) <= leaf_size:  # 叶节点\n",
    "        return BallNode(center, radius, points=points)\n",
    "\n",
    "    left_indices = np.random.choice(len(points), len(points) // 2, replace=False)\n",
    "    left_points = points[left_indices]\n",
    "    right_points = np.delete(points, left_indices, axis=0)\n",
    "\n",
    "    left_child = build_ball_tree(left_points)\n",
    "    right_child = build_ball_tree(right_points)\n",
    "\n",
    "    return BallNode(center, radius, left=left_child, right=right_child)\n",
    "\n",
    "def print_ball_tree(node, depth=0):\n",
    "    if node is None:\n",
    "        return\n",
    "\n",
    "    print(\"  \" * depth, f\"Center: {node.center}, Radius: {node.radius}\")\n",
    "\n",
    "    if node.points is not None:\n",
    "        print(\"  \" * (depth + 1), \"Leaf Points:\", node.points)\n",
    "    else:\n",
    "        print(\"  \" * (depth + 1), \"Left Child:\")\n",
    "        print_ball_tree(node.left, depth + 2)\n",
    "        print(\"  \" * (depth + 1), \"Right Child:\")\n",
    "        print_ball_tree(node.right, depth + 2)\n",
    "\n",
    "np.random.seed(0)\n",
    "points = np.random.rand(100, 2)  # 生成随机数据点\n",
    "\n",
    "global leaf_size\n",
    "leaf_size = 5  # 叶节点中允许的数据点数量\n",
    "\n",
    "ball_tree = build_ball_tree(points)\n",
    "print(\"Ball Tree construction completed.\")\n",
    "print(\"Printing Ball Tree Structure:\")\n",
    "print_ball_tree(ball_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method, Parameter and Attribute\n",
    "\n",
    "### NearestNeighbors\n",
    "\n",
    "- Method\n",
    "  - fit\n",
    "  - predict\n",
    "  - kneighbors\n",
    "    \n",
    "    Return the distances and indices of the nearest neighbor in a muliti-dimensino array. The output will includes the sample itself.\n",
    "\n",
    "  - kneighbors_graph()\n",
    "\n",
    "    Get a sparse graph showing the connections between neighboring points.\n",
    "\n",
    "- Parameter\n",
    "  - n_neighbors\n",
    "    \n",
    "    Assign how many neighbors to check\n",
    "\n",
    "  - radius\n",
    "     \n",
    "    Range of parameter space to use by default for radius_neighbors queries.\n",
    "\n",
    "  - leaf_size\n",
    "\n",
    "    Leaf size passed to BallTree or KDTree. \n",
    "\n",
    "  - metric\n",
    "\n",
    "    [see](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.distance_metrics.html#sklearn.metrics.pairwise.distance_metrics)\n",
    "\n",
    "  - p\n",
    "\n",
    "    Parameter for the Minkowski metric from sklearn.metrics.pairwise.pairwise_distances.\n",
    "\n",
    "  - metric_params\n",
    "\n",
    "    Additional keyword arguments for the metric function.\n",
    "\n",
    "  - n_jobs\n",
    "\n",
    "    The number of parallel jobs to run for neighbors search. \n",
    "    \n",
    "    This parameter is used to specify how many concurrent processes or threads should be used for routines that are parallelized with joblib.\n",
    "\n",
    "  - algorithm\n",
    "    - ball_tree\n",
    "    - kd_tree\n",
    "    - brute\n",
    "    - auto\n",
    "\n",
    "- Attributes\n",
    "  - effective_metric_\n",
    "  - effective_metric_params_\n",
    "  - n_features_in_\n",
    "  - feature_names_in_\n",
    "  - n_samples_fit_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KDTree\n",
    "\n",
    "- Parameter\n",
    "  - leaf_size\n",
    "  - metric\n",
    "- Attributes\n",
    "  - data: The train data\n",
    "\n",
    "For more information see [api](https://scikit-learn.org/stable/modules/classes.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BallTree\n",
    "\n",
    "The same to KDTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest Centroid Classifier\n",
    "\n",
    "The NearestCentroid classifier is a simple algorithm that represents each class by the centroid of its members. In effect, this makes it similar to the label updating phase of the KMeans algorithm. It also has no parameters to choose, making it **a good baseline classifier**. It does, however, **suffer on non-convex classes**, as well as when classes have **drastically different variances**, as **equal variance in all dimensions is assumed**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestCentroid\n",
    "import numpy as np\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "y = np.array([1, 1, 1, 2, 2, 2])\n",
    "clf = NearestCentroid()\n",
    "clf.fit(X, y)\n",
    "print(clf.predict([[-0.8, -1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest Shrunken Centroid\n",
    "\n",
    "The NearestCentroid classifier has a shrink_threshold parameter, which implements the nearest shrunken centroid classifier. In effect, the value of each feature for each centroid is divided by the within-class variance of that feature. The feature values are then reduced by shrink_threshold. Most notably, if a particular feature value crosses zero, it is set to zero. In effect, this removes the feature from affecting the classification. This is useful, for example, for **removing noisy features**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "# we only take the first two features. We could avoid this ugly\n",
    "# slicing by using a two-dim dataset\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "\n",
    "# Create color maps\n",
    "cmap_light = ListedColormap([\"orange\", \"cyan\", \"cornflowerblue\"])\n",
    "cmap_bold = ListedColormap([\"darkorange\", \"c\", \"darkblue\"])\n",
    "\n",
    "for shrinkage in [None, 0.2]:\n",
    "    # we create an instance of Nearest Centroid Classifier and fit the data.\n",
    "    clf = NearestCentroid(shrink_threshold=shrinkage)\n",
    "    clf.fit(X, y)\n",
    "    y_pred = clf.predict(X)\n",
    "    print(shrinkage, np.mean(y == y_pred))\n",
    "\n",
    "    _, ax = plt.subplots()\n",
    "    DecisionBoundaryDisplay.from_estimator(\n",
    "        clf, X, cmap=cmap_light, ax=ax, response_method=\"predict\"\n",
    "    )\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor=\"k\", s=20)\n",
    "    plt.title(\"3-Class classification (shrink_threshold=%r)\" % shrinkage)\n",
    "    plt.axis(\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest Neighbors Transformer\n",
    "\n",
    "See [guide](https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neighborhood Components Analysis\n",
    "\n",
    "Neighborhood Components Analysis (NCA, NeighborhoodComponentsAnalysis) is a distance metric learning algorithm which aims to **improve the accuracy of nearest neighbors classification** compared to the standard Euclidean distance. The algorithm directly maximizes a stochastic variant of the leave-one-out k-nearest neighbors (KNN) score on the training set. It **can also learn a low-dimensional linear projection of data** that can be used for **data visualization** and **fast classification**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import (NeighborhoodComponentsAnalysis,KNeighborsClassifier),\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "nca = NeighborhoodComponentsAnalysis(random_state=42)\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "nca_pipe = Pipeline([('nca', nca), ('knn', knn)])\n",
    "nca_pipe.fit(X_train, y_train)\n",
    "print(nca_pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# License: BSD 3 clause\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier, NeighborhoodComponentsAnalysis\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "n_neighbors = 1\n",
    "\n",
    "dataset = datasets.load_iris()\n",
    "X, y = dataset.data, dataset.target\n",
    "\n",
    "# we only take two features. We could avoid this ugly\n",
    "# slicing by using a two-dim dataset\n",
    "X = X[:, [0, 2]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.7, random_state=42\n",
    ")\n",
    "\n",
    "h = 0.05  # step size in the mesh\n",
    "\n",
    "# Create color maps\n",
    "cmap_light = ListedColormap([\"#FFAAAA\", \"#AAFFAA\", \"#AAAAFF\"])\n",
    "cmap_bold = ListedColormap([\"#FF0000\", \"#00FF00\", \"#0000FF\"])\n",
    "\n",
    "names = [\"KNN\", \"NCA, KNN\"]\n",
    "\n",
    "classifiers = [\n",
    "    Pipeline(\n",
    "        [\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"knn\", KNeighborsClassifier(n_neighbors=n_neighbors)),\n",
    "        ]\n",
    "    ),\n",
    "    Pipeline(\n",
    "        [\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"nca\", NeighborhoodComponentsAnalysis()),\n",
    "            (\"knn\", KNeighborsClassifier(n_neighbors=n_neighbors)),\n",
    "        ]\n",
    "    ),\n",
    "]\n",
    "\n",
    "for name, clf in zip(names, classifiers):\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "\n",
    "    _, ax = plt.subplots()\n",
    "    DecisionBoundaryDisplay.from_estimator(\n",
    "        clf,\n",
    "        X,\n",
    "        cmap=cmap_light,\n",
    "        alpha=0.8,\n",
    "        ax=ax,\n",
    "        response_method=\"predict\",\n",
    "        plot_method=\"pcolormesh\",\n",
    "        shading=\"auto\",\n",
    "    )\n",
    "\n",
    "    # Plot also the training and testing points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor=\"k\", s=20)\n",
    "    plt.title(\"{} (k = {})\".format(name, n_neighbors))\n",
    "    plt.text(\n",
    "        0.9,\n",
    "        0.1,\n",
    "        \"{:.2f}\".format(score),\n",
    "        size=15,\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        transform=plt.gca().transAxes,\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# License: BSD 3 clause\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier, NeighborhoodComponentsAnalysis\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "n_neighbors = 3\n",
    "random_state = 0\n",
    "\n",
    "# Load Digits dataset\n",
    "X, y = datasets.load_digits(return_X_y=True)\n",
    "\n",
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.5, stratify=y, random_state=random_state\n",
    ")\n",
    "\n",
    "dim = len(X[0])\n",
    "n_classes = len(np.unique(y))\n",
    "\n",
    "# Reduce dimension to 2 with PCA\n",
    "pca = make_pipeline(StandardScaler(), PCA(n_components=2, random_state=random_state))\n",
    "\n",
    "# Reduce dimension to 2 with LinearDiscriminantAnalysis\n",
    "lda = make_pipeline(StandardScaler(), LinearDiscriminantAnalysis(n_components=2))\n",
    "\n",
    "# Reduce dimension to 2 with NeighborhoodComponentAnalysis\n",
    "nca = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    NeighborhoodComponentsAnalysis(n_components=2, random_state=random_state),\n",
    ")\n",
    "\n",
    "# Use a nearest neighbor classifier to evaluate the methods\n",
    "knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "\n",
    "# Make a list of the methods to be compared\n",
    "dim_reduction_methods = [(\"PCA\", pca), (\"LDA\", lda), (\"NCA\", nca)]\n",
    "\n",
    "# plt.figure()\n",
    "for i, (name, model) in enumerate(dim_reduction_methods):\n",
    "    plt.figure()\n",
    "    # plt.subplot(1, 3, i + 1, aspect=1)\n",
    "\n",
    "    # Fit the method's model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Fit a nearest neighbor classifier on the embedded training set\n",
    "    knn.fit(model.transform(X_train), y_train)\n",
    "\n",
    "    # Compute the nearest neighbor accuracy on the embedded test set\n",
    "    acc_knn = knn.score(model.transform(X_test), y_test)\n",
    "\n",
    "    # Embed the data set in 2 dimensions using the fitted model\n",
    "    X_embedded = model.transform(X)\n",
    "\n",
    "    # Plot the projected points and show the evaluation score\n",
    "    plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=y, s=30, cmap=\"Set1\")\n",
    "    plt.title(\n",
    "        \"{}, KNN (k={})\\nTest accuracy = {:.2f}\".format(name, n_neighbors, acc_knn)\n",
    "    )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more advancerd example, see [here](https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html#sphx-glr-auto-examples-manifold-plot-lle-digits-py)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
