{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "Feature selection is a commonly used way to lower the dimension of data s.t. we can visualize the data and improve performance on high dimension data.\n",
    "\n",
    "---\n",
    "\n",
    "## Removing features with low variance\n",
    "\n",
    "VarianceThreshold is a simple baseline approach to feature selection. It removes all features whose variance doesn’t meet some threshold. By default, it removes all zero-variance features, i.e. features that have the same value in all samples.\n",
    "\n",
    "### Example\n",
    "\n",
    "As an example, suppose that we have a dataset with boolean features, and we want to remove all features that are either one or zero (on or off) in more than 80% of the samples. Boolean features are Bernoulli random variables, and the variance of such variables is given by\n",
    "\n",
    "$$Var[X]=p(1-p)$$\n",
    "\n",
    "so we can select using the threshold 0.8 * (1 - 0.8):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "X = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]\n",
    "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "sel.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate feature selection\n",
    "\n",
    "Univariate feature selection works by selecting the best features based on univariate statistical tests.\n",
    "\n",
    "### Modules\n",
    "\n",
    "- SelectKBest\n",
    "\n",
    "  SelectKBest removes all but k the highest scoring features\n",
    "\n",
    "- SelectPercentile\n",
    "\n",
    "  SelectPercentile removes all but a user-specified highest scoring percentage of features\n",
    "\n",
    "- false positive rate SelectFpr, false discovery rate SelectFdr, or family wise error SelectFwe. \n",
    "\n",
    "  Common univariate statistical tests for each feature\n",
    "\n",
    "- GenericUnivariateSelect\n",
    "\n",
    "    GenericUnivariateSelect allows to perform univariate feature selection with a configurable strategy.\n",
    "\n",
    "  For detailed description, see [api](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection)\n",
    "\n",
    "### Built-in Test\n",
    "\n",
    "- f_classif: F Test\n",
    "- mutual_info_classif: Mutual Info Test\n",
    "- chi2: chi square test\n",
    "\n",
    "F Test mainly analyze the linear dependence of the variable while the mutual info test emphasize on the dependence between the variables. Chi-square test measures dependence between stochastic variables, so using this function “weeds out” the features that are the most likely to be independent of class and therefore irrelevant for classification.\n",
    "\n",
    "If you use sparse data (i.e. data represented as sparse matrices), chi2, mutual_info_regression, mutual_info_classif will deal with the data without making it dense.\n",
    "\n",
    "For more detailed explanation see [link](https://summer-hu-92978.medium.com/complete-feature-selection-techniques-4-1-statistical-test-analysis-611ede242fa0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X.shape\n",
    "X_new = SelectKBest(f_classif, k=2).fit_transform(X, y)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_selection import f_regression, mutual_info_regression\n",
    "\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(1000, 3)\n",
    "y = X[:, 0] + np.sin(6 * np.pi * X[:, 1]) + 0.1 * np.random.randn(1000)\n",
    "\n",
    "f_test, _ = f_regression(X, y)\n",
    "f_test /= np.max(f_test)\n",
    "\n",
    "mi = mutual_info_regression(X, y)\n",
    "mi /= np.max(mi)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i in range(3):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    plt.scatter(X[:, i], y, edgecolor=\"black\", s=20)\n",
    "    plt.xlabel(\"$x_{}$\".format(i + 1), fontsize=14)\n",
    "    if i == 0:\n",
    "        plt.ylabel(\"$y$\", fontsize=14)\n",
    "    plt.title(\"F-test={:.2f}, MI={:.2f}\".format(f_test[i], mi[i]), fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import f_classif, mutual_info_classif, chi2\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "f_scores, _ = f_classif(X, y)\n",
    "mi_scores = mutual_info_classif(X, y)\n",
    "chi2_scores, _ = chi2(X, y)\n",
    "\n",
    "f_scores/= np.max(f_scores)\n",
    "mi_scores/=np.max(mi_scores)\n",
    "chi2_scores/=np.max(chi2_scores)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(len(f_scores)), f_scores, marker='o', label='F-test')\n",
    "plt.plot(range(len(mi_scores)), mi_scores, marker='s', label='Mutual Info Test')\n",
    "plt.plot(range(len(chi2_scores)), chi2_scores, marker='^', label='Chi2 Test')\n",
    "\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Feature Selection Scores Comparison')\n",
    "plt.xticks(range(X.shape[1]), [f'Feature {i}' for i in range(X.shape[1])])\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 8))\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    feature_index = i % 4\n",
    "    ax.scatter(X[:, feature_index], y, label=f'Feature {feature_index}')\n",
    "    ax.set_title(f'Scatter Plot - Feature {feature_index}')\n",
    "    ax.set_xlabel(f'Feature {feature_index}')\n",
    "    ax.set_ylabel('Class')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive feature elimination\n",
    "\n",
    "The function `RFE` will eliminate the features by recursively considering smaller and smaller sets of features. With an external estimator that assigns weights to features.  First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through any specific attribute (such as coef_, feature_importances_) or callable. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load the digits dataset\n",
    "digits = load_digits()\n",
    "X = digits.images.reshape((len(digits.images), -1))\n",
    "y = digits.target\n",
    "\n",
    "# Create the RFE object and rank each pixel\n",
    "svc = SVC(kernel=\"linear\", C=1)\n",
    "rfe = RFE(estimator=svc, n_features_to_select=3, step=1) # n_feature... eliminate until n feature left.\n",
    "# Step means the number of features got eliminated at one time.\n",
    "rfe.fit(X, y)\n",
    "ranking = rfe.ranking_.reshape(digits.images[0].shape)\n",
    "\n",
    "# Plot pixel ranking\n",
    "plt.matshow(ranking, cmap=plt.cm.Blues)\n",
    "plt.colorbar()\n",
    "plt.title(\"Ranking of pixels with RFE\")\n",
    "plt.show()\n",
    "# The white blocks is the most important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# three valueable features and two reduntant features\n",
    "X, y = make_classification(\n",
    "    n_samples=500,\n",
    "    n_features=15,\n",
    "    n_informative=3,\n",
    "    n_redundant=2,\n",
    "    n_repeated=0,\n",
    "    n_classes=8,\n",
    "    n_clusters_per_class=1,\n",
    "    class_sep=0.8,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "min_features_to_select = 1  # Minimum number of features to consider\n",
    "clf = LogisticRegression()\n",
    "cv = StratifiedKFold(5)\n",
    "\n",
    "rfecv = RFECV(\n",
    "    estimator=clf,\n",
    "    step=1,\n",
    "    cv=cv,\n",
    "    scoring=\"accuracy\",\n",
    "    min_features_to_select=min_features_to_select,\n",
    "    n_jobs=2,\n",
    ")\n",
    "rfecv.fit(X, y)\n",
    "\n",
    "print(f\"Optimal number of features: {rfecv.n_features_}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_scores = len(rfecv.cv_results_[\"mean_test_score\"])\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Mean test accuracy\")\n",
    "plt.errorbar(\n",
    "    range(min_features_to_select, n_scores + min_features_to_select),\n",
    "    rfecv.cv_results_[\"mean_test_score\"],\n",
    "    yerr=rfecv.cv_results_[\"std_test_score\"],\n",
    ")\n",
    "plt.title(\"Recursive Feature Elimination \\nwith correlated features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection using SelectFromModel\n",
    "\n",
    "SelectFromModel is a meta-transformer that can be used alongside any estimator that assigns importance to each feature through a specific attribute (such as coef_, feature_importances_) or via an importance_getter callable after fitting. The features are considered unimportant and removed if the corresponding importance of the feature values are below the provided threshold parameter. Apart from specifying the threshold numerically, there are built-in heuristics for finding a threshold using a string argument. Available heuristics are “mean”, “median” and float multiples of these like “0.1*mean”. In combination with the threshold criteria, one can use the max_features parameter to set a limit on the number of features to select."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target\n",
    "print(diabetes.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature importance from coefficients\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "ridge = RidgeCV(alphas=np.logspace(-6, 6, num=5)).fit(X, y)\n",
    "importance = np.abs(ridge.coef_)\n",
    "feature_names = np.array(diabetes.feature_names)\n",
    "plt.bar(height=importance, x=feature_names)\n",
    "plt.title(\"Feature importances via coefficients\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting features based on importance\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "threshold = np.sort(importance)[-3] + 0.01\n",
    "\n",
    "tic = time()\n",
    "sfm = SelectFromModel(ridge, threshold=threshold).fit(X, y)\n",
    "toc = time()\n",
    "print(f\"Features selected by SelectFromModel: {feature_names[sfm.get_support()]}\")\n",
    "print(f\"Done in {toc - tic:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on L1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X.shape\n",
    "lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X, y)\n",
    "model = SelectFromModel(lsvc, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X.shape\n",
    "clf = ExtraTreesClassifier(n_estimators=50)\n",
    "clf = clf.fit(X, y)\n",
    "clf.feature_importances_  \n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "X_new.shape               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Feature Selection\n",
    "\n",
    "Sequential Feature Selection [sfs] (SFS) is available in the SequentialFeatureSelector transformer. SFS can be either forward or backward:\n",
    "\n",
    "Forward-SFS is a greedy procedure that iteratively finds the best new feature to add to the set of selected features. Concretely, we initially start with zero features and find the one feature that maximizes a cross-validated score when an estimator is trained on this single feature. Once that first feature is selected, we repeat the procedure by adding a new feature to the set of selected features. The procedure stops when the desired number of selected features is reached, as determined by the n_features_to_select parameter.\n",
    "\n",
    "Backward-SFS follows the same idea but works in the opposite direction: instead of starting with no features and greedily adding features, we start with all the features and greedily remove features from the set. The direction parameter controls whether forward or backward SFS is used.\n",
    "\n",
    "In general, forward and backward selection do not yield equivalent results. Also, one may be much faster than the other depending on the requested number of selected features: if we have 10 features and ask for 7 selected features, forward selection would need to perform 7 iterations while backward selection would only need to perform 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target\n",
    "print(diabetes.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "tic_fwd = time()\n",
    "sfs_forward = SequentialFeatureSelector(\n",
    "    ridge, n_features_to_select=2, direction=\"forward\"\n",
    ").fit(X, y)\n",
    "toc_fwd = time()\n",
    "\n",
    "tic_bwd = time()\n",
    "sfs_backward = SequentialFeatureSelector(\n",
    "    ridge, n_features_to_select=2, direction=\"backward\"\n",
    ").fit(X, y)\n",
    "toc_bwd = time()\n",
    "\n",
    "print(\n",
    "    \"Features selected by forward sequential selection: \"\n",
    "    f\"{feature_names[sfs_forward.get_support()]}\"\n",
    ")\n",
    "print(f\"Done in {toc_fwd - tic_fwd:.3f}s\")\n",
    "print(\n",
    "    \"Features selected by backward sequential selection: \"\n",
    "    f\"{feature_names[sfs_backward.get_support()]}\"\n",
    ")\n",
    "print(f\"Done in {toc_bwd - tic_bwd:.3f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
