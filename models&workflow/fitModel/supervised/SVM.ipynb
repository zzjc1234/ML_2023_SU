{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM\n",
    "\n",
    "This files includes usage of svm for discrete condition.\n",
    "\n",
    "---\n",
    "\n",
    "## Intro\n",
    "\n",
    "Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection. In this file, we only introduce the svm for classifcation.\n",
    "\n",
    "## Math Principle\n",
    "\n",
    "Here is the recommended [video](https://www.youtube.com/watch?v=_PwhiWxHK8o)\n",
    "\n",
    "### Important Concepts\n",
    "\n",
    "1. Decision Boundary\n",
    "2. Widest Gutter\n",
    "3. Kernel Function\n",
    "\n",
    "### Decision Boundary\n",
    "\n",
    "Here is simply illustration on two dimensional case.\n",
    "\n",
    "![svm1](../../../src/svm.png)\n",
    "\n",
    "The vector w is perpendicular to the vector c, and we define l by equation\n",
    "$$\\langle v,w\\rangle \\geq -b$$\n",
    "Thus we obtain the decision rule with the relation\n",
    "$$\\langle v,w\\rangle +b \\geq 0$$\n",
    "\n",
    "### Widest Gutter\n",
    "\n",
    "The basic idea to separate two points is to make the gutter as large as possible. Since l is the central line, we have\n",
    "$$\\forall x_+ \\in \\text{train set postive}, \\langle x_+,w\\rangle \\geq 1 \\Leftrightarrow \\forall x_-,\\ \\langle x_-, w\\rangle + b \\leq -1$$\n",
    "\n",
    "For convenience, we define symbol $sgn(\\cdot)$, $sgn(x_+)=1$, $sgn(x_-)=-1$. The former relationship can be rewritten to\n",
    "$$sgn(x_i)(\\langle x_i, w\\rangle +b)\\geq 1, \\text{for all}\\ x_i \\in \\text{training set}.$$\n",
    "\n",
    "We define $sgn(x_i)(\\langle x_i, w \\rangle +b)=0$ if $x_i$ is in the gutter.\n",
    "\n",
    "Take $x_+$, $x_-$ on the boundary, we have the width of gutter is equal to $\\langle(x_+-x_-),\\frac{w}{||w||}\\rangle$, width=$\\frac{2}{||w||}$ with the former definition.\n",
    "\n",
    "To maximize the width, we need to minimize the $||w||$ which is equivalent to get min{$\\frac{1}{2}||w||$}. (just for mathematica convenience) Then we apply the Lagrange multiplier to get the constrained extrema.\n",
    "\n",
    "$$L=\\frac{1}{2} ||w||^2 - \\sum \\lambda_i[sgn(x_i)(\\langle w,x_i\\rangle+b)-1]$$\n",
    "\n",
    "We have\n",
    "\n",
    "$$\n",
    "\\left\\lbrace\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial w}=0\\\\\n",
    "\\frac{\\partial L}{\\partial b}=0\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "\\Rightarrow\n",
    "\n",
    "\\left\\lbrace\\begin{aligned}\n",
    "w=\\sum\\lambda_i sgn(x_i)x_i\\\\\n",
    "\\sum\\lambda_i sgn(x_i)=0\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "By plugging the result back to the original expression, we have\n",
    "$$\\displaystyle L=\\sum\\lambda_i +\\frac{1}{2}\\sum_i\\sum_j \\lambda_i\\lambda_j sgn(x_i) sgn(x_j)$$\n",
    "\n",
    "And the decision rule becomes\n",
    "\n",
    "$$\\left\\langle\\sum\\lambda_i sgn(x_i)x_i, u\\right\\rangle+b \\geq 0 \\Rightarrow x_i \\in x_+$$\n",
    "\n",
    "### Kernel Function\n",
    "\n",
    "For inseparable problems, we can use a transformation $\\phi$ s.t. $\\lbrace x_+\\rbrace$ and $\\lbrace x_-\\rbrace$ are separable. The problem is transformed to find $max\\lbrace \\langle \\phi(x_i),\\phi(u)\\rangle\\rbrace$\n",
    "\n",
    "For convenience, we define kernel fucntion $k(x_i,x_j)=\\langle\\phi(x_i),\\phi(x_j)\\rangle$\n",
    "\n",
    "---\n",
    "\n",
    "## Advantage & Disadvantage\n",
    "\n",
    "- Advantage\n",
    "  - Effective in high dimensional spaces.\n",
    "  - Still effective in cases where number of dimensions is greater than the number of samples.\n",
    "  - Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "  - Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
    "- Disadvantage\n",
    "  - If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n",
    "  - SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities, below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realization\n",
    "\n",
    "There are mainly three model for svm, SVC, NuSVC and LinearSVC. The LinearSVC is a faster model with linear kernel s.t. it doesn't accept kernerl parameter.\n",
    "\n",
    "### SVC\n",
    "\n",
    "The comparing to basic svm, the SVC introduces the penalty term C. The equation of gutter width is modified to find\n",
    "$$\\displaystyle \\mathop{min}\\limits_{\\omega, b, \\xi} \\frac{1}{2}\\lVert w \\rVert^2 + C \\sum_i \\xi_i$$\n",
    "\n",
    "subject to\n",
    "$$sgn(x_i)(\\langle w, \\phi(x_i)\\rangle+b) \\geq 1 - \\xi_i, \\ \\xi_i \\geq 0$$\n",
    "\n",
    "$\\xi$ stands for the distance from the correct boundry. For those $x_i$ is wrongly classified, the chosen hypersurface will be punished according to $\\xi_i$ and C.\n",
    "\n",
    "Similarly, by talking the Langrange multiplier, we get the constrained extrema by\n",
    "\n",
    "$$\\mathop{min}\\limits_{\\alpha}\\frac{1}{2}\\alpha^TQ\\alpha-e^T\\alpha\\ \\text{and} \\ sgn{x_i}\\alpha=0, 0\\leq \\alpha_i \\leq C$$\n",
    "\n",
    "Q is a n by n positive semidefinite matrix which $Q_{ij}=sgn(x_i)sgn(x_j)K(x_i,x_j)$. $\\alpha_i$ are called the dual coefficients.\n",
    "\n",
    "Once the optimization problem is solved, the decision rule should be\n",
    "$$\\sum sgn(x_i)\\alpha_iK(x_i,u)+b$$\n",
    "\n",
    "### NuSVC\n",
    "\n",
    "The NuSVC is the a reparameterization of SVC and therefore mathematically equivalent.\n",
    "\n",
    "We introduce a new parameter $\\nu$ (instead of C) which controls the number of support vectors and margin errors: $\\nu \\in (0,1]$ is an upper bound on the fraction of margin errors and a lower bound of the fraction of support vectors. A margin error corresponds to a sample that lies on the wrong side of its margin boundary: it is either misclassified, or it is correctly classified but does not lie beyond the margin."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
