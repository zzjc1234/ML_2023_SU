{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM\n",
    "\n",
    "This files includes usage of svm for discrete condition.\n",
    "\n",
    "---\n",
    "\n",
    "## Intro\n",
    "\n",
    "Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection. In this file, we only introduce the svm for classifcation.\n",
    "\n",
    "## Math Principle\n",
    "\n",
    "Here is the recommended [video](https://www.youtube.com/watch?v=_PwhiWxHK8o)\n",
    "\n",
    "### Important Concepts\n",
    "\n",
    "1. Decision Boundary\n",
    "2. Widest Gutter\n",
    "3. Kernel Function\n",
    "\n",
    "### Decision Boundary\n",
    "\n",
    "Here is simply illustration on two dimensional case.\n",
    "\n",
    "![svm1](../../../src/svm.png)\n",
    "\n",
    "The vector w is perpendicular to the vector c, and we define l by equation\n",
    "$$\\langle v,w\\rangle \\geq -b$$\n",
    "Thus we obtain the decision rule with the relation\n",
    "$$\\langle v,w\\rangle +b \\geq 0$$\n",
    "\n",
    "### Widest Gutter\n",
    "\n",
    "The basic idea to separate two points is to make the gutter as large as possible. Since l is the central line, we have\n",
    "$$\\forall x_+ \\in \\text{train set postive}, \\langle x_+,w\\rangle \\geq 1 \\Leftrightarrow \\forall x_-,\\ \\langle x_-, w\\rangle + b \\leq -1$$\n",
    "\n",
    "For convenience, we define symbol $sgn(\\cdot)$, $sgn(x_+)=1$, $sgn(x_-)=-1$. The former relationship can be rewritten to\n",
    "$$sgn(x_i)(\\langle x_i, w\\rangle +b)\\geq 1, \\text{for all}\\ x_i \\in \\text{training set}.$$\n",
    "\n",
    "We define $sgn(x_i)(\\langle x_i, w \\rangle +b)=0$ if $x_i$ is in the gutter.\n",
    "\n",
    "Take $x_+$, $x_-$ on the boundary, we have the width of gutter is equal to $\\langle(x_+-x_-),\\frac{w}{||w||}\\rangle$, width=$\\frac{2}{||w||}$ with the former definition. The $x_+$, $x_-$ on the boundary are called **support vectors**\n",
    "\n",
    "To maximize the width, we need to minimize the $||w||$ which is equivalent to get min{$\\frac{1}{2}||w||$}. (just for mathematica convenience) Then we apply the Lagrange multiplier to get the constrained extrema.\n",
    "\n",
    "$$L=\\frac{1}{2} ||w||^2 - \\sum \\lambda_i[sgn(x_i)(\\langle w,x_i\\rangle+b)-1]$$\n",
    "\n",
    "We have\n",
    "\n",
    "$$\n",
    "\\left\\lbrace\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial w}=0\\\\\n",
    "\\frac{\\partial L}{\\partial b}=0\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "\\Rightarrow\n",
    "\n",
    "\\left\\lbrace\\begin{aligned}\n",
    "w=\\sum\\lambda_i sgn(x_i)x_i\\\\\n",
    "\\sum\\lambda_i sgn(x_i)=0\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "By plugging the result back to the original expression, we have\n",
    "$$\\displaystyle L=\\sum\\lambda_i +\\frac{1}{2}\\sum_i\\sum_j \\lambda_i\\lambda_j sgn(x_i) sgn(x_j)$$\n",
    "\n",
    "And the decision rule becomes\n",
    "\n",
    "$$\\left\\langle\\sum\\lambda_i sgn(x_i)x_i, u\\right\\rangle+b \\geq 0 \\Rightarrow x_i \\in x_+$$\n",
    "\n",
    "### Kernel Function\n",
    "\n",
    "For inseparable problems, we can use a transformation $\\phi$ s.t. $\\lbrace x_+\\rbrace$ and $\\lbrace x_-\\rbrace$ are separable. The problem is transformed to find $max\\lbrace \\langle \\phi(x_i),\\phi(u)\\rangle\\rbrace$\n",
    "\n",
    "For convenience, we define kernel fucntion $k(x_i,x_j)=\\langle\\phi(x_i),\\phi(x_j)\\rangle$\n",
    "\n",
    "---\n",
    "\n",
    "## Advantage & Disadvantage\n",
    "\n",
    "- Advantage\n",
    "  - Effective in high dimensional spaces.\n",
    "  - Still effective in cases where number of dimensions is greater than the number of samples.\n",
    "  - Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "  - Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
    "- Disadvantage\n",
    "  - If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n",
    "  - SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities, below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realization\n",
    "\n",
    "There are mainly three model for svm, SVC, NuSVC and LinearSVC. The LinearSVC is a faster model with linear kernel s.t. it doesn't accept kernerl parameter.\n",
    "\n",
    "### SVC\n",
    "\n",
    "The comparing to basic svm, the SVC introduces the penalty term C. The equation of gutter width is modified to find\n",
    "$$\\displaystyle \\mathop{min}\\limits_{\\omega, b, \\xi} \\frac{1}{2}\\lVert w \\rVert^2 + C \\sum_i \\xi_i$$\n",
    "\n",
    "subject to\n",
    "$$sgn(x_i)(\\langle w, \\phi(x_i)\\rangle+b) \\geq 1 - \\xi_i, \\ \\xi_i \\geq 0$$\n",
    "\n",
    "$\\xi$ stands for the distance from the correct boundry. For those $x_i$ is wrongly classified, the chosen hypersurface will be punished according to $\\xi_i$ and C.\n",
    "\n",
    "Similarly, by talking the Langrange multiplier, we get the constrained extrema by\n",
    "\n",
    "$$\\mathop{min}\\limits_{\\alpha}\\frac{1}{2}\\alpha^TQ\\alpha-e^T\\alpha\\ \\text{and} \\ sgn{x_i}\\alpha=0, 0\\leq \\alpha_i \\leq C$$\n",
    "\n",
    "Q is a n by n positive semidefinite matrix which $Q_{ij}=sgn(x_i)sgn(x_j)K(x_i,x_j)$. $\\alpha_i$ are called the dual coefficients.\n",
    "\n",
    "Once the optimization problem is solved, the decision rule should be\n",
    "$$\\sum sgn(x_i)\\alpha_iK(x_i,u)+b$$\n",
    "\n",
    "### NuSVC\n",
    "\n",
    "The NuSVC is the a reparameterization of SVC and therefore mathematically equivalent.\n",
    "\n",
    "We introduce a new parameter $\\nu$ (instead of C) which controls the number of support vectors and margin errors: $\\nu \\in (0,1]$ is an upper bound on the fraction of margin errors and a lower bound of the fraction of support vectors. A margin error corresponds to a sample that lies on the wrong side of its margin boundary: it is either misclassified, or it is correctly classified but does not lie beyond the margin.\n",
    "\n",
    "---\n",
    "\n",
    "## Methods, Parameters and Attributes\n",
    "\n",
    "### Methods\n",
    "\n",
    "- .fit()\n",
    "- .predict()\n",
    "- .decision_function()\n",
    "  \n",
    "  This method can gives per-class scores for each sample (or a single score per sample in the binary case). When the constructor option probability is set to True, class membership probability estimates (from the methods predict_proba and predict_log_proba) are enabled.\n",
    "\n",
    "  The result can be used to get the confidence scores and provide information for multi-model condition.\n",
    "\n",
    "  It is recommended to use the decision_function method instead of the rest\n",
    "\n",
    "- predict_proba\n",
    "- predict_log_proba \n",
    "\n",
    "### Parameters\n",
    "\n",
    "- kernel\n",
    "  - linear\n",
    "\n",
    "    $\\langle x,x`\\rangle$\n",
    "\n",
    "  - polynomial\n",
    "\n",
    "    $(\\gamma\\langle x, x'\\rangle+r)^d$, d is specified by parameter `degree`, r by `coef0`\n",
    "    \n",
    "  - rbf\n",
    "\n",
    "    $exp(-\\gamma\\lVert x-x'\\rVert^2)$, where $\\gamma$ is specified by parameter gamma, must be greater than 0.\n",
    "\n",
    "  - sigmoid\n",
    "\n",
    "    $tanh(\\gamma\\langle x,x'\\rangle+r)$\n",
    "- decision_function_shape\n",
    "  - ovo\n",
    "  - ovr\n",
    "- C\n",
    "  \n",
    "  C is 1 by default and it’s a reasonable default choice. If you have a lot of noisy observations you should decrease it: decreasing C corresponds to more regularization.\n",
    "\n",
    "  LinearSVC and LinearSVR are less sensitive to C when it becomes large, and prediction results stop improving after a certain threshold. Meanwhile, larger C values will take more time to train, sometimes up to 10 times longer\n",
    "\n",
    "- gamma\n",
    "- dual(LinearSVC)\n",
    "- cache_size\n",
    "\n",
    "  For SVC, SVR, NuSVC and NuSVR, the size of the kernel cache has a strong impact on run times for larger problems. If you have enough RAM available, it is recommended to set cache_size to a higher value than the default of 200(MB), such as 500(MB) or 1000(MB).\n",
    "\n",
    "- shrinking\n",
    "  \n",
    "  We found that if the number of iterations is large, then shrinking can shorten the training time. However, if we loosely solve the optimization problem (e.g., by using a large stopping tolerance), the code without using shrinking may be much faster\n",
    "\n",
    "- nu\n",
    "  \n",
    "  Parameter nu in NuSVC/OneClassSVM/NuSVR approximates the fraction of training errors and support vectors.\n",
    "\n",
    "- random_state\n",
    "  \n",
    "  If probability is set to False these estimators are not random and random_state has no effect on the results.\n",
    "\n",
    "- class_weight\n",
    "\n",
    "  In SVC, if the data is unbalanced (e.g. many positive and few negative), set class_weight='balanced' and/or try different penalty parameters C.\n",
    "\n",
    "### Attributes\n",
    "\n",
    "- support_vectors_\n",
    "\n",
    "  support vectors\n",
    "\n",
    "- support_\n",
    "\n",
    "  indices of support vectors\n",
    "\n",
    "- n_support_\n",
    "\n",
    "  number of support vectors for each class\n",
    "\n",
    "### Tips\n",
    "\n",
    "It is highly recommended to scale your data. For example, scale each attribute on the input vector X to [0,1] or [-1,+1], or standardize it to have mean 0 and variance 1. Note that the same scaling must be applied to the test vector to obtain meaningful results.\n",
    "\n",
    "When training an SVM with the Radial Basis Function (RBF) kernel, two parameters must be considered: C and gamma. The parameter C, common to all SVM kernels, trades off misclassification of training examples against simplicity of the decision surface. A low C makes the decision surface smooth, while a high C aims at classifying all training examples correctly. gamma defines how much influence a single training example has. The larger gamma is, the closer other examples must be to be affected. Using `GridSearchCV` is recommended.\n",
    "\n",
    "## Sample Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "X = [[0, 0], [1, 1]]\n",
    "y = [0, 1]\n",
    "clf = svm.SVC()\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([[2., 2.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0.]\n",
      " [1. 1.]]\n",
      "[0 1]\n",
      "[1 1]\n"
     ]
    }
   ],
   "source": [
    "# get support vectors\n",
    "print(clf.support_vectors_)\n",
    "# get indices of support vectors\n",
    "print(clf.support_)\n",
    "# get number of support vectors for each class\n",
    "print(clf.n_support_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = make_pipeline(StandardScaler(), SVC())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Strategy\n",
    "\n",
    "SVC and NuSVC use one vs one strategy to handle the multiclass classification. User can change the strategy by using 'decision_function_shape' parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[0], [1], [2], [3]]\n",
    "Y = [0, 1, 2, 3]\n",
    "clf = svm.SVC(decision_function_shape='ovo')\n",
    "clf.fit(X, Y)\n",
    "dec = clf.decision_function([[1]])\n",
    "dec.shape[1] # 4 classes: 4*3/2 = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.decision_function_shape = \"ovr\"\n",
    "dec = clf.decision_function([[1]])\n",
    "dec.shape[1] # 4 classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, LinearSVC implements “one-vs-the-rest” multi-class strategy, thus training n_classes models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_clf = svm.LinearSVC(dual=\"auto\")\n",
    "lin_clf.fit(X, Y)\n",
    "dec = lin_clf.decision_function([[1]])\n",
    "dec.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An illustration for kernel selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svc = svm.SVC(kernel='linear')\n",
    "linear_svc.kernel\n",
    "rbf_svc = svm.SVC(kernel='rbf')\n",
    "rbf_svc.kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unbalanced problems\n",
    "\n",
    "Find the optimal separating hyperplane using an SVC for classes that are unbalanced.\n",
    "\n",
    "We first find the separating plane with a plain SVC and then plot (dashed) the separating hyperplane with automatically correction for unbalanced classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "# we create two clusters of random points\n",
    "n_samples_1 = 1000\n",
    "n_samples_2 = 100\n",
    "centers = [[0.0, 0.0], [2.0, 2.0]]\n",
    "clusters_std = [1.5, 0.5]\n",
    "X, y = make_blobs(\n",
    "    n_samples=[n_samples_1, n_samples_2],\n",
    "    centers=centers,\n",
    "    cluster_std=clusters_std,\n",
    "    random_state=0,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# fit the model and get the separating hyperplane\n",
    "clf = svm.SVC(kernel=\"linear\", C=1.0)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# fit the model and get the separating hyperplane using weighted classes\n",
    "wclf = svm.SVC(kernel=\"linear\", class_weight={1: 10})\n",
    "wclf.fit(X, y)\n",
    "\n",
    "# plot the samples\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, edgecolors=\"k\")\n",
    "\n",
    "# plot the decision functions for both classifiers\n",
    "ax = plt.gca()\n",
    "disp = DecisionBoundaryDisplay.from_estimator(\n",
    "    clf,\n",
    "    X,\n",
    "    plot_method=\"contour\",\n",
    "    colors=\"k\",\n",
    "    levels=[0],\n",
    "    alpha=0.5,\n",
    "    linestyles=[\"-\"],\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "# plot decision boundary and margins for weighted classes\n",
    "wdisp = DecisionBoundaryDisplay.from_estimator(\n",
    "    wclf,\n",
    "    X,\n",
    "    plot_method=\"contour\",\n",
    "    colors=\"r\",\n",
    "    levels=[0],\n",
    "    alpha=0.5,\n",
    "    linestyles=[\"-\"],\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "plt.legend(\n",
    "    [disp.surface_.collections[0], wdisp.surface_.collections[0]],\n",
    "    [\"non weighted\", \"weighted\"],\n",
    "    loc=\"upper right\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot decision function of a weighted dataset, where the size of points is proportional to its weight.\n",
    "\n",
    "The sample weighting rescales the C parameter, which means that the classifier puts more emphasis on getting these points right. The effect might often be subtle. To emphasize the effect here, we particularly weight outliers, making the deformation of the decision boundary very visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "def plot_decision_function(classifier, sample_weight, axis, title):\n",
    "    # plot the decision function\n",
    "    xx, yy = np.meshgrid(np.linspace(-4, 5, 500), np.linspace(-4, 5, 500))\n",
    "\n",
    "    Z = classifier.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # plot the line, the points, and the nearest vectors to the plane\n",
    "    axis.contourf(xx, yy, Z, alpha=0.75, cmap=plt.cm.bone)\n",
    "    axis.scatter(\n",
    "        X[:, 0],\n",
    "        X[:, 1],\n",
    "        c=y,\n",
    "        s=100 * sample_weight,\n",
    "        alpha=0.9,\n",
    "        cmap=plt.cm.bone,\n",
    "        edgecolors=\"black\",\n",
    "    )\n",
    "\n",
    "    axis.axis(\"off\")\n",
    "    axis.set_title(title)\n",
    "\n",
    "\n",
    "# we create 20 points\n",
    "np.random.seed(0)\n",
    "X = np.r_[np.random.randn(10, 2) + [1, 1], np.random.randn(10, 2)]\n",
    "y = [1] * 10 + [-1] * 10\n",
    "sample_weight_last_ten = abs(np.random.randn(len(X)))\n",
    "sample_weight_constant = np.ones(len(X))\n",
    "# and bigger weights to some outliers\n",
    "sample_weight_last_ten[15:] *= 5\n",
    "sample_weight_last_ten[9] *= 15\n",
    "\n",
    "# Fit the models.\n",
    "\n",
    "# This model does not take into account sample weights.\n",
    "clf_no_weights = svm.SVC(gamma=1)\n",
    "clf_no_weights.fit(X, y)\n",
    "\n",
    "# This other model takes into account some dedicated sample weights.\n",
    "clf_weights = svm.SVC(gamma=1)\n",
    "clf_weights.fit(X, y, sample_weight=sample_weight_last_ten)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "plot_decision_function(\n",
    "    clf_no_weights, sample_weight_constant, axes[0], \"Constant weights\"\n",
    ")\n",
    "plot_decision_function(clf_weights, sample_weight_last_ten, axes[1], \"Modified weights\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
