{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM\n",
    "\n",
    "This files includes usage of svm for discrete condition.\n",
    "\n",
    "---\n",
    "\n",
    "## Intro\n",
    "\n",
    "Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection. In this file, we only introduce the svm for classifcation.\n",
    "\n",
    "## Math Principle\n",
    "\n",
    "Here is the recommended [video](https://www.youtube.com/watch?v=_PwhiWxHK8o)\n",
    "\n",
    "### Important Concepts\n",
    "\n",
    "1. Decision Boundary\n",
    "2. Widest Gutter\n",
    "3. Kernel Function\n",
    "\n",
    "### Decision Boundary\n",
    "\n",
    "Here is simply illustration on two dimensional case.\n",
    "\n",
    "![svm1](../../../src/svm.png)\n",
    "\n",
    "The vector w is perpendicular to the vector c, and we define l by equation\n",
    "$$\\langle v,w\\rangle \\geq -b$$\n",
    "Thus we obtain the decision rule with the relation\n",
    "$$\\langle v,w\\rangle +b \\geq 0$$\n",
    "\n",
    "### Widest Gutter\n",
    "\n",
    "The basic idea to separate two points is to make the gutter as large as possible. Since l is the central line, we have\n",
    "$$\\forall x_+ \\in \\text{train set postive}, \\langle x_+,w\\rangle \\geq 1 \\Leftrightarrow \\forall x_-,\\ \\langle x_-, w\\rangle + b \\leq -1$$\n",
    "\n",
    "For convenience, we define symbol $sgn(\\cdot)$, $sgn(x_+)=1$, $sgn(x_-)=-1$. The former relationship can be rewritten to\n",
    "$$sgn(x_i)(\\langle x_i, w\\rangle +b)\\geq 1, \\text{for all}\\ x_i \\in \\text{training set}.$$\n",
    "\n",
    "We define $sgn(x_i)(\\langle x_i, w \\rangle +b)=0$ if $x_i$ is in the gutter.\n",
    "\n",
    "Take $x_+$, $x_-$ on the boundary, we have the width of gutter is equal to $\\langle(x_+-x_-),\\frac{w}{||w||}\\rangle$, width=$\\frac{2}{||w||}$ with the former definition. The $x_+$, $x_-$ on the boundary are called **support vectors**\n",
    "\n",
    "To maximize the width, we need to minimize the $||w||$ which is equivalent to get min{$\\frac{1}{2}||w||$}. (just for mathematica convenience) Then we apply the Lagrange multiplier to get the constrained extrema.\n",
    "\n",
    "$$L=\\frac{1}{2} ||w||^2 - \\sum \\lambda_i[sgn(x_i)(\\langle w,x_i\\rangle+b)-1]$$\n",
    "\n",
    "We have\n",
    "\n",
    "$$\n",
    "\\left\\lbrace\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial w}=0\\\\\n",
    "\\frac{\\partial L}{\\partial b}=0\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "\\Rightarrow\n",
    "\n",
    "\\left\\lbrace\\begin{aligned}\n",
    "w=\\sum\\lambda_i sgn(x_i)x_i\\\\\n",
    "\\sum\\lambda_i sgn(x_i)=0\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "By plugging the result back to the original expression, we have\n",
    "$$\\displaystyle L=\\sum\\lambda_i +\\frac{1}{2}\\sum_i\\sum_j \\lambda_i\\lambda_j sgn(x_i) sgn(x_j)$$\n",
    "\n",
    "And the decision rule becomes\n",
    "\n",
    "$$\\left\\langle\\sum\\lambda_i sgn(x_i)x_i, u\\right\\rangle+b \\geq 0 \\Rightarrow x_i \\in x_+$$\n",
    "\n",
    "### Kernel Function\n",
    "\n",
    "For inseparable problems, we can use a transformation $\\phi$ s.t. $\\lbrace x_+\\rbrace$ and $\\lbrace x_-\\rbrace$ are separable. The problem is transformed to find $max\\lbrace \\langle \\phi(x_i),\\phi(u)\\rangle\\rbrace$\n",
    "\n",
    "For convenience, we define kernel fucntion $k(x_i,x_j)=\\langle\\phi(x_i),\\phi(x_j)\\rangle$\n",
    "\n",
    "---\n",
    "\n",
    "## Advantage & Disadvantage\n",
    "\n",
    "- Advantage\n",
    "  - Effective in high dimensional spaces.\n",
    "  - Still effective in cases where number of dimensions is greater than the number of samples.\n",
    "  - Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "  - Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
    "- Disadvantage\n",
    "  - If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n",
    "  - SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities, below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realization\n",
    "\n",
    "There are mainly three model for svm, SVC, NuSVC and LinearSVC. The LinearSVC is a faster model with linear kernel s.t. it doesn't accept kernerl parameter.\n",
    "\n",
    "### SVC\n",
    "\n",
    "The comparing to basic svm, the SVC introduces the penalty term C. The equation of gutter width is modified to find\n",
    "$$\\displaystyle \\mathop{min}\\limits_{\\omega, b, \\xi} \\frac{1}{2}\\lVert w \\rVert^2 + C \\sum_i \\xi_i$$\n",
    "\n",
    "subject to\n",
    "$$sgn(x_i)(\\langle w, \\phi(x_i)\\rangle+b) \\geq 1 - \\xi_i, \\ \\xi_i \\geq 0$$\n",
    "\n",
    "$\\xi$ stands for the distance from the correct boundry. For those $x_i$ is wrongly classified, the chosen hypersurface will be punished according to $\\xi_i$ and C.\n",
    "\n",
    "Similarly, by talking the Langrange multiplier, we get the constrained extrema by\n",
    "\n",
    "$$\\mathop{min}\\limits_{\\alpha}\\frac{1}{2}\\alpha^TQ\\alpha-e^T\\alpha\\ \\text{and} \\ sgn{x_i}\\alpha=0, 0\\leq \\alpha_i \\leq C$$\n",
    "\n",
    "Q is a n by n positive semidefinite matrix which $Q_{ij}=sgn(x_i)sgn(x_j)K(x_i,x_j)$. $\\alpha_i$ are called the dual coefficients.\n",
    "\n",
    "Once the optimization problem is solved, the decision rule should be\n",
    "$$\\sum sgn(x_i)\\alpha_iK(x_i,u)+b$$\n",
    "\n",
    "### NuSVC\n",
    "\n",
    "The NuSVC is the a reparameterization of SVC and therefore mathematically equivalent.\n",
    "\n",
    "We introduce a new parameter $\\nu$ (instead of C) which controls the number of support vectors and margin errors: $\\nu \\in (0,1]$ is an upper bound on the fraction of margin errors and a lower bound of the fraction of support vectors. A margin error corresponds to a sample that lies on the wrong side of its margin boundary: it is either misclassified, or it is correctly classified but does not lie beyond the margin.\n",
    "\n",
    "---\n",
    "\n",
    "## Methods, Parameters and Attributes\n",
    "\n",
    "### Methods\n",
    "\n",
    "- .fit()\n",
    "- .predict()\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- kernel\n",
    "  - linear\n",
    "\n",
    "    $\\langle x,x`\\rangle$\n",
    "\n",
    "  - polynomial\n",
    "\n",
    "    $(\\gamma\\langle x, x'\\rangle+r)^d$, d is specified by parameter `degree`, r by `coef0`\n",
    "    \n",
    "  - rbf\n",
    "\n",
    "    $exp(-\\gamma\\lVert x-x'\\rVert^2)$, where $\\gamma$ is specified by parameter gamma, must be greater than 0.\n",
    "\n",
    "  - sigmoid\n",
    "\n",
    "    $tanh(\\gamma\\langle x,x'\\rangle+r)$\n",
    "- decision_function_shape\n",
    "  - ovo\n",
    "  - ovr\n",
    "- C\n",
    "- gamma\n",
    "- dual(LinearSVC)\n",
    "\n",
    "### Attributes\n",
    "\n",
    "- support_vectors_\n",
    "\n",
    "  support vectors\n",
    "\n",
    "- support_\n",
    "\n",
    "  indices of support vectors\n",
    "\n",
    "- n_support_\n",
    "\n",
    "  number of support vectors for each class\n",
    "\n",
    "## Sample Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "X = [[0, 0], [1, 1]]\n",
    "y = [0, 1]\n",
    "clf = svm.SVC()\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([[2., 2.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0.]\n",
      " [1. 1.]]\n",
      "[0 1]\n",
      "[1 1]\n"
     ]
    }
   ],
   "source": [
    "# get support vectors\n",
    "print(clf.support_vectors_)\n",
    "# get indices of support vectors\n",
    "print(clf.support_)\n",
    "# get number of support vectors for each class\n",
    "print(clf.n_support_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Strategy\n",
    "\n",
    "SVC and NuSVC use one vs one strategy to handle the multiclass classification. User can change the strategy by using 'decision_function_shape' parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[0], [1], [2], [3]]\n",
    "Y = [0, 1, 2, 3]\n",
    "clf = svm.SVC(decision_function_shape='ovo')\n",
    "clf.fit(X, Y)\n",
    "dec = clf.decision_function([[1]])\n",
    "dec.shape[1] # 4 classes: 4*3/2 = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.decision_function_shape = \"ovr\"\n",
    "dec = clf.decision_function([[1]])\n",
    "dec.shape[1] # 4 classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, LinearSVC implements “one-vs-the-rest” multi-class strategy, thus training n_classes models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_clf = svm.LinearSVC(dual=\"auto\")\n",
    "lin_clf.fit(X, Y)\n",
    "dec = lin_clf.decision_function([[1]])\n",
    "dec.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An illustration for kernel selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svc = svm.SVC(kernel='linear')\n",
    "linear_svc.kernel\n",
    "rbf_svc = svm.SVC(kernel='rbf')\n",
    "rbf_svc.kernel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
