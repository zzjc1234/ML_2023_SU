{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM\n",
    "\n",
    "This files includes usage of svm for discrete condition.\n",
    "\n",
    "---\n",
    "\n",
    "## Intro\n",
    "\n",
    "Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection. In this file, we only introduce the svm for classifcation.\n",
    "\n",
    "## Math Principle\n",
    "\n",
    "Here is the recommended [video](https://www.youtube.com/watch?v=_PwhiWxHK8o)\n",
    "\n",
    "### Important Concepts\n",
    "\n",
    "1. Decision Boundary\n",
    "2. Widest Gutter\n",
    "3. Kernel Function\n",
    "\n",
    "### Decision Boundary\n",
    "\n",
    "Here is simply illustration on two dimensional case.\n",
    "\n",
    "![svm1](../../../src/svm.png)\n",
    "\n",
    "The vector w is perpendicular to the vector c, and we define l by equation\n",
    "$$\\langle v,w\\rangle \\geq -b$$\n",
    "Thus we obtain the decision rule with the relation\n",
    "$$\\langle v,w\\rangle +b \\geq 0$$\n",
    "\n",
    "### Widest Gutter\n",
    "\n",
    "The basic idea to separate two points is to make the gutter as large as possible. Since l is the central line, we have\n",
    "$$\\forall x_+ \\in \\text{train set postive}, \\langle x_+,w\\rangle \\geq t,\\ t \\in \\mathbb{R}^+ \\Leftrightarrow \\forall x_-,\\ \\langle x_-, w\\rangle + b \\leq -t$$\n",
    "\n",
    "For convenience, we define symbol $sgn(\\cdot)$, $sgn(x_+)=1$, $sgn(x_-)=-1$. The former relationship can be rewritten to\n",
    "$$sgn(x_i)(\\langle x_i, w\\rangle +b)\\geq t, \\text{for all}\\ x_i \\in \\text{training set}.$$\n",
    "\n",
    "We define $sgn(x_i)(\\langle x_i, w \\rangle +b)=0$ if $x_i$ is in the gutter.\n",
    "\n",
    "Take $x_+$, $x_-$ on the boundary, we have the width of gutter is equal to $\\langle(x_+-x_-),\\frac{w}{||w||}\\rangle$, width=$frac{2t}{||w||} with the former definition.\n",
    "\n",
    "To maximize the width, we need to minimize the $||w||$ which is equivalent to get min{$\\frac{1}{2}||w||$}. (just for mathematica convenience) Then we apply the Lagrange multiplier to get the constrained extrema.\n",
    "\n",
    "$$L=\\frac{1}{2} ||w||^2 - \\sum \\lambda_i[sgn(x_i)(\\langle w,x_i\\rangle+b)-1]$$\n",
    "\n",
    "We have\n",
    "\n",
    "$$\n",
    "\\left\\lbrace\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial w}=0\\\\\n",
    "\\frac{\\partial L}{\\partial b}=0\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "\\Rightarrow\n",
    "\n",
    "\\left\\lbrace\\begin{aligned}\n",
    "w=\\sum\\lambda_i sgn(x_i)x_i\\\\\n",
    "\\sum\\lambda_i sgn(x_i)=0\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "By plugging the result back to the original expression, we have\n",
    "$$\\displaystyle L=\\sum\\lambda_i +\\frac{1}{2}\\sum_i\\sum_j \\lambda_i\\lambda_j sgn(x_i) sgn(x_j)$$\n",
    "\n",
    "And the decision rule becomes\n",
    "\n",
    "$$\\left\\langle\\sum\\lambda_i sgn(x_i)x_i, u\\right\\rangle+b \\geq 0 \\Rightarrow x_i \\in x_+$$\n",
    "\n",
    "### Kernel Function\n",
    "\n",
    "For inseparable problems, we can use a transformation $\\phi$ s.t. $\\lbrace x_+\\rbrace$ and $\\lbrace x_-\\rbrace$ are separable. The problem is transformed to find $max\\lbrace \\langle \\phi(x_i),\\phi(u)\\rangle\\rbrace$\n",
    "\n",
    "For convenience, we define kernel fucntion $k(x_i,x_j)=\\langle\\phi(x_i),\\phi(x_j)\\rangle$\n",
    "\n",
    "---\n",
    "\n",
    "## Advantage & Disadvantage\n",
    "\n",
    "- Advantage\n",
    "  - Effective in high dimensional spaces.\n",
    "  - Still effective in cases where number of dimensions is greater than the number of samples.\n",
    "  - Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "  - Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
    "- Disadvantage\n",
    "  - If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n",
    "  - SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities, below)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
