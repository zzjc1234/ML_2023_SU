{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-supervised Learning\n",
    "\n",
    "Semi-supervised learning is a situation in which in your training data some of the samples are not labeled. The semi-supervised estimators in sklearn.semi_supervised are able to make use of this additional unlabeled data to better capture the shape of the underlying data distribution and generalize better to new samples. These algorithms can perform well when we have a very small amount of labeled points and a large amount of unlabeled points.\n",
    "\n",
    "## Unlabeled entries in y\n",
    "\n",
    "It is important to assign an identifier to unlabeled points along with the labeled data when training the model with the fit method. The identifier that this implementation uses is the integer value. Note that for string labels, the dtype of y should be object so that it can contain both strings and integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Training\n",
    "\n",
    "This self-training implementation is based on Yarowskyâ€™s [1] algorithm. Using this algorithm, a given supervised classifier can function as a semi-supervised classifier, allowing it to learn from unlabeled data.\n",
    "\n",
    "SelfTrainingClassifier can be called with any classifier that implements predict_proba, passed as the parameter base_classifier. In each iteration, the base_classifier predicts labels for the unlabeled samples and adds a subset of these labels to the labeled dataset.\n",
    "\n",
    "The choice of this subset is determined by the selection criterion. This selection can be done using a threshold on the prediction probabilities, or by choosing the k_best samples according to the prediction probabilities.\n",
    "\n",
    "The labels used for the final fit as well as the iteration in which each sample was labeled are available as attributes. The optional max_iter parameter specifies how many times the loop is executed at most.\n",
    "\n",
    "The max_iter parameter may be set to None, causing the algorithm to iterate until all samples have labels or no new samples are selected in that iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of varying threshold for self-training\n",
    "\n",
    "# Authors: Oliver Rausch <rauscho@ethz.ch>\n",
    "# License: BSD\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.semi_supervised import SelfTrainingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "n_splits = 3\n",
    "\n",
    "X, y = datasets.load_breast_cancer(return_X_y=True)\n",
    "X, y = shuffle(X, y, random_state=42)\n",
    "y_true = y.copy()\n",
    "y[50:] = -1\n",
    "total_samples = y.shape[0]\n",
    "\n",
    "base_classifier = SVC(probability=True, gamma=0.001, random_state=42)\n",
    "\n",
    "x_values = np.arange(0.4, 1.05, 0.05)\n",
    "x_values = np.append(x_values, 0.99999)\n",
    "scores = np.empty((x_values.shape[0], n_splits))\n",
    "amount_labeled = np.empty((x_values.shape[0], n_splits))\n",
    "amount_iterations = np.empty((x_values.shape[0], n_splits))\n",
    "\n",
    "for i, threshold in enumerate(x_values):\n",
    "    self_training_clf = SelfTrainingClassifier(base_classifier, threshold=threshold)\n",
    "\n",
    "    # We need manual cross validation so that we don't treat -1 as a separate\n",
    "    # class when computing accuracy\n",
    "    skfolds = StratifiedKFold(n_splits=n_splits)\n",
    "    for fold, (train_index, test_index) in enumerate(skfolds.split(X, y)):\n",
    "        X_train = X[train_index]\n",
    "        y_train = y[train_index]\n",
    "        X_test = X[test_index]\n",
    "        y_test = y[test_index]\n",
    "        y_test_true = y_true[test_index]\n",
    "\n",
    "        self_training_clf.fit(X_train, y_train)\n",
    "\n",
    "        # The amount of labeled samples that at the end of fitting\n",
    "        amount_labeled[i, fold] = (\n",
    "            total_samples\n",
    "            - np.unique(self_training_clf.labeled_iter_, return_counts=True)[1][0]\n",
    "        )\n",
    "        # The last iteration the classifier labeled a sample in\n",
    "        amount_iterations[i, fold] = np.max(self_training_clf.labeled_iter_)\n",
    "\n",
    "        y_pred = self_training_clf.predict(X_test)\n",
    "        scores[i, fold] = accuracy_score(y_test_true, y_pred)\n",
    "\n",
    "\n",
    "ax1 = plt.subplot(211)\n",
    "ax1.errorbar(\n",
    "    x_values, scores.mean(axis=1), yerr=scores.std(axis=1), capsize=2, color=\"b\"\n",
    ")\n",
    "ax1.set_ylabel(\"Accuracy\", color=\"b\")\n",
    "ax1.tick_params(\"y\", colors=\"b\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.errorbar(\n",
    "    x_values,\n",
    "    amount_labeled.mean(axis=1),\n",
    "    yerr=amount_labeled.std(axis=1),\n",
    "    capsize=2,\n",
    "    color=\"g\",\n",
    ")\n",
    "ax2.set_ylim(bottom=0)\n",
    "ax2.set_ylabel(\"Amount of labeled samples\", color=\"g\")\n",
    "ax2.tick_params(\"y\", colors=\"g\")\n",
    "\n",
    "ax3 = plt.subplot(212, sharex=ax1)\n",
    "ax3.errorbar(\n",
    "    x_values,\n",
    "    amount_iterations.mean(axis=1),\n",
    "    yerr=amount_iterations.std(axis=1),\n",
    "    capsize=2,\n",
    "    color=\"b\",\n",
    ")\n",
    "ax3.set_ylim(bottom=0)\n",
    "ax3.set_ylabel(\"Amount of iterations\")\n",
    "ax3.set_xlabel(\"Threshold\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Propagation\n",
    "\n",
    "Label propagation denotes a few variations of semi-supervised graph inference algorithms.\n",
    "\n",
    "A few features available in this model:\n",
    "\n",
    "- Used for classification tasks\n",
    "- Kernel methods to project data into alternate dimensional spaces\n",
    "\n",
    "scikit-learn provides two label propagation models: LabelPropagation and LabelSpreading. Both work by constructing a similarity graph over all items in the input dataset.\n",
    "\n",
    "- LabelPropagation\n",
    "  - Change the weight of the true ground labeled data to some degree.\n",
    "    \n",
    "    algorithm performs hard clamping of input labels, which means $\\alpha=0$. This clamping factor can be relaxed, to say $\\alpha=0.2$, which means that we will always retain 80 percent of our original label distribution, but the algorithm gets to change its confidence of the distribution within 20 percent.\n",
    "\n",
    "- LabelSpreading\n",
    "  - Minimizes a loss function that has regularization properties, as such it is often more robust to noise. The algorithm iterates on a modified version of the original graph and normalizes the edge weights by computing the normalized graph Laplacian matrix. This procedure is also used in Spectral clustering.\n",
    "\n",
    "- Kernel Function\n",
    "  - RBF\n",
    "  - KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision boundary of semi-supervised classifiers versus SVM on the Iris dataset\n",
    "\n",
    "# Authors: Clay Woolam   <clay@woolam.org>\n",
    "#          Oliver Rausch <rauscho@ethz.ch>\n",
    "# License: BSD\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.semi_supervised import LabelSpreading, SelfTrainingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "\n",
    "# step size in the mesh\n",
    "h = 0.02\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "y_rand = rng.rand(y.shape[0])\n",
    "y_30 = np.copy(y)\n",
    "y_30[y_rand < 0.3] = -1  # set random samples to be unlabeled\n",
    "y_50 = np.copy(y)\n",
    "y_50[y_rand < 0.5] = -1\n",
    "# we create an instance of SVM and fit out data. We do not scale our\n",
    "# data since we want to plot the support vectors\n",
    "ls30 = (LabelSpreading().fit(X, y_30), y_30, \"Label Spreading 30% data\")\n",
    "ls50 = (LabelSpreading().fit(X, y_50), y_50, \"Label Spreading 50% data\")\n",
    "ls100 = (LabelSpreading().fit(X, y), y, \"Label Spreading 100% data\")\n",
    "\n",
    "# the base classifier for self-training is identical to the SVC\n",
    "base_classifier = SVC(kernel=\"rbf\", gamma=0.5, probability=True)\n",
    "st30 = (\n",
    "    SelfTrainingClassifier(base_classifier).fit(X, y_30),\n",
    "    y_30,\n",
    "    \"Self-training 30% data\",\n",
    ")\n",
    "st50 = (\n",
    "    SelfTrainingClassifier(base_classifier).fit(X, y_50),\n",
    "    y_50,\n",
    "    \"Self-training 50% data\",\n",
    ")\n",
    "\n",
    "rbf_svc = (SVC(kernel=\"rbf\", gamma=0.5).fit(X, y), y, \"SVC with rbf kernel\")\n",
    "\n",
    "# create a mesh to plot in\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "color_map = {-1: (1, 1, 1), 0: (0, 0, 0.9), 1: (1, 0, 0), 2: (0.8, 0.6, 0)}\n",
    "\n",
    "classifiers = (ls30, st30, ls50, st50, ls100, rbf_svc)\n",
    "for i, (clf, y_train, title) in enumerate(classifiers):\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    plt.subplot(3, 2, i + 1)\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Plot also the training points\n",
    "    colors = [color_map[y] for y in y_train]\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=colors, edgecolors=\"black\")\n",
    "\n",
    "    plt.title(title)\n",
    "\n",
    "plt.suptitle(\"Unlabeled points are colored white\", y=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "n_samples = 200\n",
    "X, y = make_circles(n_samples=n_samples, shuffle=False)\n",
    "outer, inner = 0, 1\n",
    "labels = np.full(n_samples, -1.0)\n",
    "labels[0] = outer\n",
    "labels[-1] = inner\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.scatter(\n",
    "    X[labels == outer, 0],\n",
    "    X[labels == outer, 1],\n",
    "    color=\"navy\",\n",
    "    marker=\"s\",\n",
    "    lw=0,\n",
    "    label=\"outer labeled\",\n",
    "    s=10,\n",
    ")\n",
    "plt.scatter(\n",
    "    X[labels == inner, 0],\n",
    "    X[labels == inner, 1],\n",
    "    color=\"c\",\n",
    "    marker=\"s\",\n",
    "    lw=0,\n",
    "    label=\"inner labeled\",\n",
    "    s=10,\n",
    ")\n",
    "plt.scatter(\n",
    "    X[labels == -1, 0],\n",
    "    X[labels == -1, 1],\n",
    "    color=\"darkorange\",\n",
    "    marker=\".\",\n",
    "    label=\"unlabeled\",\n",
    ")\n",
    "plt.legend(scatterpoints=1, shadow=False, loc=\"center\")\n",
    "_ = plt.title(\"Raw data (2 classes=outer and inner)\")\n",
    "\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "\n",
    "label_spread = LabelSpreading(kernel=\"knn\", alpha=0.8)\n",
    "label_spread.fit(X, labels)\n",
    "\n",
    "output_labels = label_spread.transduction_\n",
    "output_label_array = np.asarray(output_labels)\n",
    "outer_numbers = np.where(output_label_array == outer)[0]\n",
    "inner_numbers = np.where(output_label_array == inner)[0]\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.scatter(\n",
    "    X[outer_numbers, 0],\n",
    "    X[outer_numbers, 1],\n",
    "    color=\"navy\",\n",
    "    marker=\"s\",\n",
    "    lw=0,\n",
    "    s=10,\n",
    "    label=\"outer learned\",\n",
    ")\n",
    "plt.scatter(\n",
    "    X[inner_numbers, 0],\n",
    "    X[inner_numbers, 1],\n",
    "    color=\"c\",\n",
    "    marker=\"s\",\n",
    "    lw=0,\n",
    "    s=10,\n",
    "    label=\"inner learned\",\n",
    ")\n",
    "plt.legend(scatterpoints=1, shadow=False, loc=\"center\")\n",
    "plt.title(\"Labels learned with Label Spreading (KNN)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "rng = np.random.RandomState(2)\n",
    "indices = np.arange(len(digits.data))\n",
    "rng.shuffle(indices)\n",
    "\n",
    "X = digits.data[indices[:340]]\n",
    "y = digits.target[indices[:340]]\n",
    "images = digits.images[indices[:340]]\n",
    "\n",
    "n_total_samples = len(y)\n",
    "n_labeled_points = 40\n",
    "\n",
    "indices = np.arange(n_total_samples)\n",
    "\n",
    "unlabeled_set = indices[n_labeled_points:]\n",
    "\n",
    "y_train = np.copy(y)\n",
    "y_train[unlabeled_set] = -1\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "\n",
    "lp_model = LabelSpreading(gamma=0.25, max_iter=20)\n",
    "lp_model.fit(X, y_train)\n",
    "predicted_labels = lp_model.transduction_[unlabeled_set]\n",
    "true_labels = y[unlabeled_set]\n",
    "\n",
    "print(\n",
    "    \"Label Spreading model: %d labeled & %d unlabeled points (%d total)\"\n",
    "    % (n_labeled_points, n_total_samples - n_labeled_points, n_total_samples)\n",
    ")\n",
    "\n",
    "print(classification_report(true_labels, predicted_labels))\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    true_labels, predicted_labels, labels=lp_model.classes_\n",
    ")\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "pred_entropies = stats.distributions.entropy(lp_model.label_distributions_.T)\n",
    "\n",
    "uncertainty_index = np.argsort(pred_entropies)[-10:]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f = plt.figure(figsize=(7, 5))\n",
    "for index, image_index in enumerate(uncertainty_index):\n",
    "    image = images[image_index]\n",
    "\n",
    "    sub = f.add_subplot(2, 5, index + 1)\n",
    "    sub.imshow(image, cmap=plt.cm.gray_r)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    sub.set_title(\n",
    "        \"predict: %i\\ntrue: %i\" % (lp_model.transduction_[image_index], y[image_index])\n",
    "    )\n",
    "\n",
    "f.suptitle(\"Learning with small amount of labeled data\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
