{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass and multioutput algorithms\n",
    "\n",
    "Since most of the algorithm in sklearn has a built-in multiclass algorithm. To see detailed information about models, see [here](https://scikit-learn.org/stable/modules/multiclass.html)\n",
    "\n",
    "---\n",
    "\n",
    "The difference between class and lable is that one sample can only belong to one class but belong to many lables. Although all models in sklearn can handle multiclass problems, we can use the sklearn.multiclass to change the strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y = np.array(['apple', 'pear', 'apple', 'orange'])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "y = np.array(['apple', 'pear', 'apple', 'orange'])\n",
    "y_dense = LabelBinarizer().fit_transform(y)\n",
    "print(y_dense)\n",
    "from scipy import sparse\n",
    "y_sparse = sparse.csr_matrix(y_dense)\n",
    "print(y_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OvR\n",
    "\n",
    "OVR is short for one vs rest strategy. The strategy consists in fitting one classifier per class. For each classifier, the class is fitted against all the other classes. In addition to its computational efficiency (only n_classes classifiers are needed), one advantage of this approach is its **interpretability**. Since each class is represented by one and only one classifier, it is possible to gain knowledge about the class by inspecting its corresponding classifier. This is the most commonly used strategy and is a fair default choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "OneVsRestClassifier(LinearSVC(dual=\"auto\", random_state=0)).fit(X, y).predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OneVsRestClassifier also supports multilabel classification. To use this feature, feed the classifier an indicator matrix, in which cell [i, j] indicates the presence of label j in sample i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authors: Vlad Niculae, Mathieu Blondel\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "def plot_hyperplane(clf, min_x, max_x, linestyle, label):\n",
    "    # get the separating hyperplane\n",
    "    w = clf.coef_[0]\n",
    "    a = -w[0] / w[1]\n",
    "    xx = np.linspace(min_x - 5, max_x + 5)  # make sure the line is long enough\n",
    "    yy = a * xx - (clf.intercept_[0]) / w[1]\n",
    "    plt.plot(xx, yy, linestyle, label=label)\n",
    "\n",
    "\n",
    "def plot_subfigure(X, Y, subplot, title, transform):\n",
    "    if transform == \"pca\":\n",
    "        X = PCA(n_components=2).fit_transform(X)\n",
    "    elif transform == \"cca\":\n",
    "        X = CCA(n_components=2).fit(X, Y).transform(X)\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    min_x = np.min(X[:, 0])\n",
    "    max_x = np.max(X[:, 0])\n",
    "\n",
    "    min_y = np.min(X[:, 1])\n",
    "    max_y = np.max(X[:, 1])\n",
    "\n",
    "    classif = OneVsRestClassifier(SVC(kernel=\"linear\"))\n",
    "    classif.fit(X, Y)\n",
    "\n",
    "    plt.subplot(2, 2, subplot)\n",
    "    plt.title(title)\n",
    "\n",
    "    zero_class = np.where(Y[:, 0])\n",
    "    one_class = np.where(Y[:, 1])\n",
    "    plt.scatter(X[:, 0], X[:, 1], s=40, c=\"gray\", edgecolors=(0, 0, 0))\n",
    "    plt.scatter(\n",
    "        X[zero_class, 0],\n",
    "        X[zero_class, 1],\n",
    "        s=160,\n",
    "        edgecolors=\"b\",\n",
    "        facecolors=\"none\",\n",
    "        linewidths=2,\n",
    "        label=\"Class 1\",\n",
    "    )\n",
    "    plt.scatter(\n",
    "        X[one_class, 0],\n",
    "        X[one_class, 1],\n",
    "        s=80,\n",
    "        edgecolors=\"orange\",\n",
    "        facecolors=\"none\",\n",
    "        linewidths=2,\n",
    "        label=\"Class 2\",\n",
    "    )\n",
    "\n",
    "    plot_hyperplane(\n",
    "        classif.estimators_[0], min_x, max_x, \"k--\", \"Boundary\\nfor class 1\"\n",
    "    )\n",
    "    plot_hyperplane(\n",
    "        classif.estimators_[1], min_x, max_x, \"k-.\", \"Boundary\\nfor class 2\"\n",
    "    )\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "\n",
    "    plt.xlim(min_x - 0.5 * max_x, max_x + 0.5 * max_x)\n",
    "    plt.ylim(min_y - 0.5 * max_y, max_y + 0.5 * max_y)\n",
    "    if subplot == 2:\n",
    "        plt.xlabel(\"First principal component\")\n",
    "        plt.ylabel(\"Second principal component\")\n",
    "        plt.legend(loc=\"upper left\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "X, Y = make_multilabel_classification(\n",
    "    n_classes=2, n_labels=1, allow_unlabeled=True, random_state=1\n",
    ")\n",
    "\n",
    "plot_subfigure(X, Y, 1, \"With unlabeled samples + CCA\", \"cca\")\n",
    "plot_subfigure(X, Y, 2, \"With unlabeled samples + PCA\", \"pca\")\n",
    "\n",
    "X, Y = make_multilabel_classification(\n",
    "    n_classes=2, n_labels=1, allow_unlabeled=False, random_state=1\n",
    ")\n",
    "\n",
    "plot_subfigure(X, Y, 3, \"Without unlabeled samples + CCA\", \"cca\")\n",
    "plot_subfigure(X, Y, 4, \"Without unlabeled samples + PCA\", \"pca\")\n",
    "\n",
    "plt.subplots_adjust(0.04, 0.02, 0.97, 0.94, 0.09, 0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OvO\n",
    "\n",
    "OneVsOneClassifier constructs one classifier per pair of classes. At prediction time, the class which received the most votes is selected. In the event of a tie (among two classes with an equal number of votes), it selects the class with the highest aggregate classification confidence by summing over the pair-wise classification confidence levels computed by the underlying binary classifiers.\n",
    "\n",
    "Since it requires to fit n_classes * (n_classes - 1) / 2 classifiers, this method is usually **slower than one-vs-the-rest**, due to its O(n_classes^2) complexity. However, this method may be **advantageous for algorithms such as kernel algorithms which don’t scale well with n_samples**. This is because each individual learning problem only involves a small subset of the data whereas, with one-vs-the-rest, the complete dataset is used n_classes times. The decision function is the result of a monotonic transformation of the one-versus-one classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "OneVsOneClassifier(LinearSVC(dual=\"auto\", random_state=0)).fit(X, y).predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OutputCodeClassifier\n",
    "\n",
    "Error-Correcting Output Code-based strategies are fairly different from one-vs-the-rest and one-vs-one. With these strategies, each class is represented in a Euclidean space, where each dimension can only be 0 or 1. Another way to put it is that each class is represented by a binary code (an array of 0 and 1). The matrix which keeps track of the location/code of each class is called the code book. The code size is the dimensionality of the aforementioned space. Intuitively, each class should be represented by a code as unique as possible and a good code book should be designed to optimize classification accuracy.\n",
    "\n",
    "At fitting time, one binary classifier per bit in the code book is fitted. At prediction time, the classifiers are used to project new points in the class space and the class closest to the points is chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.multiclass import OutputCodeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "print(len(set(y)))\n",
    "import numpy as np\n",
    "clf = OutputCodeClassifier(LinearSVC(dual=\"auto\", random_state=0),code_size=2, random_state=0)\n",
    "# code_size determines the length of classifier. e.g. for a data set with 3 class, the classfier will have 6 number.\n",
    "print(clf.fit(X, y).predict(X))\n",
    "print(clf.fit(X,y).code_book_) #This will explain the mechanism of code_size\n",
    "# code_book record the code of used classifier.\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "y = np.where(y == 2, 1, y)\n",
    "print(len(set(y)))\n",
    "print(y)\n",
    "clf.fit(X, y).predict(X)\n",
    "print(clf.fit(X,y).code_book_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This demo shows code_size's effect on the accuracy of model\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.multiclass import OutputCodeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Change label 2 to label 1\n",
    "y[y == 2] = 1\n",
    "\n",
    "# Create OutputCodeClassifier with different code sizes\n",
    "code_sizes = [1, 2, 3]\n",
    "for code_size in code_sizes:\n",
    "    clf = OutputCodeClassifier(LinearSVC(dual=\"auto\", random_state=0), code_size=code_size, random_state=0)\n",
    "    predictions = clf.fit(X, y).predict(X)\n",
    "    accuracy = accuracy_score(y, predictions)\n",
    "    print(\"Code size:\", code_size, \"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilabel\n",
    "\n",
    "Multilabel classification (closely related to multioutput classification) is a classification task labeling each sample with m labels from n_classes possible classes, where m can be 0 to n_classes inclusive. This can be thought of as predicting properties of a sample that are not mutually exclusive. Formally, a binary output is assigned to each class, for every sample. Positive classes are indicated with 1 and negative classes with 0 or -1. It is thus comparable to running n_classes binary classification tasks, for example with MultiOutputClassifier. This approach treats each label independently whereas multilabel classifiers may treat the multiple classes simultaneously, accounting for correlated behavior among them.\n",
    "\n",
    "For example, prediction of the topics relevant to a text document or video. The document or video may be about one of ‘religion’, ‘politics’, ‘finance’ or ‘education’, several of the topic classes or all of the topic classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([[1, 0, 0, 1], [0, 0, 1, 1], [0, 0, 0, 0]])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "y_sparse = sparse.csr_matrix(y)\n",
    "print(y_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ClassifierChain\n",
    "\n",
    "see [here](https://scikit-learn.org/stable/modules/generated/sklearn.multioutput.ClassifierChain.html#sklearn.multioutput.ClassifierChain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass-multioutput classification\n",
    "\n",
    "Multiclass-multioutput classification (also known as multitask classification) is a classification task which labels each sample with a set of non-binary properties. Both the number of properties and the number of classes per property is greater than 2. A single estimator thus handles several joint classification tasks. I.e. one classifier can track two different features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "X, y1 = make_classification(n_samples=10, n_features=100,n_informative=30, n_classes=3,random_state=1)\n",
    "y2 = shuffle(y1, random_state=1)\n",
    "y3 = shuffle(y1, random_state=2)\n",
    "Y = np.vstack((y1, y2, y3)).T\n",
    "n_samples, n_features = X.shape # 10,100\n",
    "n_outputs = Y.shape[1] # 3\n",
    "n_classes = 3\n",
    "forest = RandomForestClassifier(random_state=1)\n",
    "multi_target_forest = MultiOutputClassifier(forest, n_jobs=2)\n",
    "multi_target_forest.fit(X, Y).predict(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
