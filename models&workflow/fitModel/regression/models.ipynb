{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models based on sklearn\n",
    "Referring to  \n",
    "[Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook)  \n",
    "[Official Document](https://scikit-learn.org/stable/user_guide.html)  \n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic sklearn commands "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- multiclass classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression -- continuous\n",
    "\n",
    "- 判断线性or非线性\n",
    "  - 残差图\n",
    "  - 散点图\n",
    "  - 训练多个模型看准确率\n",
    "- 回归指标  \n",
    "  **损失函数常用 MAE、MSE、RMSE**   \n",
    "  **性能评估指标常用R-square**（R-square就是在用均值预测的标准下衡量模型的预测性能）\n",
    "  - 越接近0越好\n",
    "    - SSE (残差平方和)  $$\\sum{|y_i-\\hat{y_i}|^2}$$\n",
    "    - MAE (平均绝对误差)  $$\\sum{\\frac{|y_i-\\hat{y_i}|}{n}}$$\n",
    "    - MSE (均方误差)  $$\\frac{SSE}{n}=\\sum{\\frac{|y_i-\\hat{y_i}|^2}{n}}$$\n",
    "    - RMSE (均方根误差) $$\\sum{\\sqrt{\\frac{|y_i-\\hat{y_i}|^2}{n}}}$$\n",
    "  - 越接近1越好 \n",
    "    - R-squared (确定系数)\n",
    "      - SSR：预测数据与原始均值的平方和\n",
    "      - SST（残差）：原始数据与原始均值的平方和\n",
    "      - SSE：预测数据与原始数据的平方和\n",
    "    - Adjusted R-squared  (调整R方)  \n",
    "\n",
    "    只要增加了更多的变量，R-squared 要么保持不变，要么增加。  \n",
    "    如果增加更多无意义的变量，Adjusted R-squared 会下降；如果加入的特征值是显著的，则adjusted R-squared也会上升。  \n",
    "    在单变量线性回归中，R-squared和adjusted R-squared是一致的。  \n",
    "\n",
    "    **结论**：如果单变量线性回归，则使用 R-squared评估；多变量，则使用adjusted R-squared。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error , r2_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 线性\n",
    "[sklearn线性回归 岭回归 lasso回归示例代码](https://zhuanlan.zhihu.com/p/165493873)\n",
    "- 标准线性回归\n",
    "  - 特征之间相互独立 --> or, 多重共线性\n",
    "- 岭回归\n",
    "  - 可以解决特征数>样本量的问题，有效防止过拟合\n",
    "  - 可以处理高度相关的数据，变量间存在共线性（最小二乘回归得到的系数不稳定，方差很大）\n",
    "- lasso回归  \n",
    "    lasso 容易使得部分权重取 0，所以可以用其做 feature selection，lasso 的名字so即为 selection operator。权重为 0 的 feature 对回归问题没有贡献，直接去掉权重为 0 的 feature，模型的输出值不变。（参考[lasso回归和岭回归](https://www.cnblogs.com/wuliytTaotao/p/10837533.html)）  \n",
    "    lasso 更容易使得权重变为 0，而 ridge 更容易使得权重接近 0。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### 标准线性回归\n",
    "- 语法：\n",
    "    ```python\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    ```\n",
    "- Parameters:  \n",
    "\n",
    "    **fit_intercept**：有无截距  \n",
    "    **copy_X**：True赋值，False覆写  \n",
    "    **normalize**：是否归一化  \n",
    "    **n_jobs**：default=1，使用CPU数，-1为使用所有CPU  \n",
    "\n",
    "- Attributes:  \n",
    "\n",
    "    **coef_**：如果label有两个，即y值有两列，那么是一个2D的array  \n",
    "    **intercept_**：截距  \n",
    "\n",
    "- Methods:\n",
    "\n",
    "    **fit(X, y, sample_weight)**: 训练模型  \n",
    "    **get_params(X, y, sample_weight)**: 获取参数  \n",
    "    **set_params(para)**: 设置参数  \n",
    "    **predict(X)**：预测数据  \n",
    "    **score(X)**：评估，得到R方  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple linear regression 一元\n",
    "from sklearn.linear_model import LinearRegression as lm\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 定义样本和特征数量\n",
    "num_sample=1000\n",
    "num_feature = 1\n",
    "# num_feature = 2\n",
    "\n",
    "# 生成一些样本和特征\n",
    "ideal_coef = -3\n",
    "# ideal_coef = [2,-3]\n",
    "ideal_intercept = 5\n",
    "feature=np.random.normal(size=(num_sample,num_feature))\n",
    "label=ideal_coef*feature + ideal_intercept + np.random.normal(size=(num_sample,num_feature))\n",
    "# label=ideal_coef[0]*feature[:,0] + ideal_coef[1]*feature[:,1] + ideal_intercept + np.random.normal(size=(num_sample,))\n",
    "\n",
    "# Slice x into training/testing sets\n",
    "X_train = feature[:-100,:]\n",
    "X_test = feature[-100:,:]\n",
    "# Slice y into training/testing sets\n",
    "y_train = label[:-100]\n",
    "y_test = label[-100:]\n",
    "\n",
    "# fit model\n",
    "model = lm()\n",
    "model.fit(X_train,y_train)\n",
    "y_predict=model.predict(X_test)\n",
    "print(\"mean_square_error:%.2f\" %mean_squared_error(y_test,y_predict))\n",
    "print('R-squared: %.2f' %r2_score(y_test, y_predict))\n",
    "# print('R-squared: %.2f' %model.score(X_test, y_test)) 结果同上\n",
    "print(\"coefficient of the model:%.2f\" %model.coef_)\n",
    "print(\"intercept of the model:%.2f\" %model.intercept_)\n",
    "\n",
    "# Plot outputs\n",
    "plt.scatter(X_test, y_test, color='black')\n",
    "plt.plot(X_test, y_predict, color='green', linewidth=3)\n",
    "\n",
    "# 残差\n",
    "for idx, x in enumerate(X_test):\n",
    "    plt.plot([x, x], [y_test[idx], y_predict[idx]], 'g-')\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple linear regression  二元\n",
    "# !!!!! 注意多元线性回归，feature.reshape(1000,)，一定要统一为二维\n",
    "from sklearn.linear_model import LinearRegression as lm\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "import numpy as np\n",
    "\n",
    "# 定义样本和特征数量\n",
    "num_sample=1000\n",
    "# num_feature = 1\n",
    "num_feature = 2\n",
    "\n",
    "# 生成一些样本和特征\n",
    "# ideal_coef = -3\n",
    "ideal_coef = [2,-3]\n",
    "ideal_intercept = 5\n",
    "feature=np.random.normal(size=(num_sample,num_feature))\n",
    "# label=ideal_coef*feature + ideal_intercept + np.random.normal(size=(num_sample,num_feature))\n",
    "label=ideal_coef[0]*feature[:,0] + ideal_coef[1]*feature[:,1] + ideal_intercept + np.random.normal(size=(num_sample,))\n",
    "\n",
    "# Slice x into training/testing sets\n",
    "X_train = feature[:-100,:]\n",
    "X_test = feature[-100:,:]\n",
    "# Slice y into training/testing sets\n",
    "y_train = label[:-100]\n",
    "y_test = label[-100:]\n",
    "\n",
    "# fit model\n",
    "model = lm()\n",
    "model.fit(X_train,y_train)\n",
    "y_predict=model.predict(X_test)\n",
    "print(\"mean_square_error:%.2f\" %mean_squared_error(y_test,y_predict))\n",
    "print('R-squared: %.2f' %r2_score(y_test, y_predict))\n",
    "# print('R-squared: %.2f' %model.score(X_test, y_test)) 结果同上\n",
    "# print(\"coefficient of the model:%.2f\" %model.coef_)\n",
    "print(\"coefficient of the model:%.2f, %.2f\" %(model.coef_[0],model.coef_[1]))\n",
    "print(\"intercept of the model:%.2f\" %model.intercept_)\n",
    "for i, prediction in enumerate(y_predict):\n",
    "    print('Predicted: %s, Target: %s' % (prediction, y_test[i]))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ridge Regression & Lasso Regression\n",
    "- 语法：\n",
    "    ```python\n",
    "    from sklearn.linear_model import Ridge\n",
    "    from sklearn.linear_model import Lasso\n",
    "    from sklearn.linear_model import RidgeCV\n",
    "    from sklearn.linear_model import LassoCV\n",
    "    ```\n",
    "- Parameters:  \n",
    "\n",
    "    **alpha/alphas(CV)**：正则项系数，初始值为1，数值越大，则对复杂模型的惩罚力度越大。 \n",
    "    **cv**： cross-validation generator，默认留一。  \n",
    "    **scoring**：判定best_score的指标。\n",
    "    **random_state**：种子相同，可以复现。\n",
    "    **max_iter**：最大迭代次数。  \n",
    "    **fit_intercept**：有无截距  \n",
    "    **copy_X**：True赋值，False覆写  \n",
    "    **normalize**：是否归一化  \n",
    "    **n_jobs**：default=1，使用CPU数，-1为使用所有CPU  \n",
    "\n",
    "    更多参数见 [sklearn线性回归参数](https://blog.csdn.net/VariableX/article/details/107166602)\n",
    "\n",
    "- 调参方法：  \n",
    "\n",
    "  1. 给定alpha较小的值，例如0.1。\n",
    "  2. 根据验证集准确率以10倍为单位增大或者减小参数值。[0.001, 0.01, 0.1, 1, 10, 100]\n",
    "  3. 在找到合适的数量级后，在此数量级上微调。  \n",
    "    <br>\n",
    "- Attributes (CV)  \n",
    "    **alpha_**  \n",
    "    **best_score_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RidgeCV\n",
    "from sklearn.linear_model import RidgeCV as lm\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "import numpy as np\n",
    "\n",
    "# 定义样本和特征数量\n",
    "num_sample=1000\n",
    "# num_feature = 1\n",
    "num_feature = 2\n",
    "\n",
    "# 生成一些样本和特征\n",
    "# ideal_coef = -3\n",
    "ideal_coef = [2,-3]\n",
    "ideal_intercept = 5\n",
    "feature=np.random.normal(size=(num_sample,num_feature))\n",
    "# label=ideal_coef*feature + ideal_intercept + np.random.normal(size=(num_sample,num_feature))\n",
    "label=ideal_coef[0]*feature[:,0] + ideal_coef[1]*feature[:,1] + ideal_intercept + np.random.normal(size=(num_sample,))\n",
    "\n",
    "# Slice x into training/testing sets\n",
    "X_train = feature[:-100,:]\n",
    "X_test = feature[-100:,:]\n",
    "# Slice y into training/testing sets\n",
    "y_train = label[:-100]\n",
    "y_test = label[-100:]\n",
    "\n",
    "# fit model\n",
    "model = lm(alphas = [0.01,0.1,1,10,100], scoring = 'r2')\n",
    "model.fit(X_train,y_train)\n",
    "y_predict=model.predict(X_test)\n",
    "print(\"mean_square_error:%.2f\" %mean_squared_error(y_test,y_predict))\n",
    "print('R-squared: %.2f' %r2_score(y_test, y_predict))\n",
    "# print('R-squared: %.2f' %model.score(X_test, y_test)) 结果同上\n",
    "# print(\"coefficient of the model:%.2f\" %model.coef_)\n",
    "print(\"coefficient of the model:%.2f, %.2f\" %(model.coef_[0],model.coef_[1]))\n",
    "print(\"intercept of the model:%.2f\" %model.intercept_)\n",
    "print(\"best alpha:%.2f\" %model.alpha_)\n",
    "print(\"best score:%.2f\" %model.best_score_)\n",
    "for i, prediction in enumerate(y_predict):\n",
    "    print('Predicted: %s, Target: %s' % (prediction, y_test[i]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Elastic Net 弹性网\n",
    "##### Perceptron 感知机"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 非线性\n",
    "\n",
    "- 线性模型解决非线性问题\n",
    "  - 分箱\n",
    "  - 多项式回归\n",
    "- Decision tree\n",
    "- Random forest\n",
    "- SVM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 线性模型-分箱\n",
    "\n",
    "- 语法：\n",
    "    ```python\n",
    "    from sklearn.preprocessing import KBinsDiscretizer\n",
    "    ```\n",
    "- Parameters:  \n",
    "\n",
    "    **n_bins**：几个箱子  \n",
    "    **strategy**：{‘uniform’, ‘quantile’, ‘kmeans’}，default=’quantile’，宽度一样，点数一样，kmeans聚类\n",
    "    **random_state**：种子相同，可以复现。  \n",
    "\n",
    "- Attributes:  \n",
    "\n",
    "    **bin_edges_**：边界array  \n",
    "    **n_bins_**：个数int array  \n",
    "    **n_features_in_**  \n",
    "    **feature_names_in_**  \n",
    "\n",
    "- Methods:\n",
    "\n",
    "    **fit(X)**  \n",
    "    **transform(X)**  \n",
    "    **inverse_transform(Xt)**  \n",
    "    **fit_transform(X, y)**  \n",
    "    **get_params(X, y, sample_weight)**: 获取参数  \n",
    "    **set_params(para)**: 设置参数    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [5.]\n",
      " [4.]\n",
      " [6.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [7.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [5.]\n",
      " [3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [5.]\n",
      " [4.]\n",
      " [5.]\n",
      " [3.]\n",
      " [5.]\n",
      " [3.]\n",
      " [7.]\n",
      " [1.]\n",
      " [5.]\n",
      " [6.]\n",
      " [5.]\n",
      " [2.]\n",
      " [5.]\n",
      " [3.]\n",
      " [4.]\n",
      " [0.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]\n",
      " [7.]\n",
      " [8.]\n",
      " [5.]\n",
      " [0.]\n",
      " [6.]\n",
      " [6.]\n",
      " [2.]\n",
      " [7.]\n",
      " [6.]\n",
      " [5.]\n",
      " [6.]\n",
      " [4.]\n",
      " [6.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [5.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [5.]\n",
      " [3.]\n",
      " [3.]\n",
      " [5.]\n",
      " [5.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [5.]\n",
      " [3.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [5.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [5.]\n",
      " [1.]\n",
      " [7.]\n",
      " [4.]\n",
      " [6.]\n",
      " [4.]\n",
      " [7.]\n",
      " [1.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [6.]\n",
      " [2.]\n",
      " [5.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [6.]\n",
      " [6.]\n",
      " [4.]\n",
      " [2.]\n",
      " [5.]\n",
      " [4.]\n",
      " [5.]\n",
      " [5.]\n",
      " [7.]\n",
      " [7.]\n",
      " [4.]\n",
      " [7.]\n",
      " [3.]\n",
      " [4.]\n",
      " [6.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]\n",
      " [5.]\n",
      " [6.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [5.]\n",
      " [5.]\n",
      " [4.]\n",
      " [5.]\n",
      " [5.]\n",
      " [5.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [6.]\n",
      " [2.]\n",
      " [6.]\n",
      " [6.]\n",
      " [5.]\n",
      " [4.]\n",
      " [3.]\n",
      " [5.]\n",
      " [6.]\n",
      " [6.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [5.]\n",
      " [2.]\n",
      " [9.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [6.]\n",
      " [3.]\n",
      " [0.]\n",
      " [2.]\n",
      " [5.]\n",
      " [5.]\n",
      " [2.]\n",
      " [6.]\n",
      " [3.]\n",
      " [5.]\n",
      " [5.]\n",
      " [3.]\n",
      " [5.]\n",
      " [4.]\n",
      " [7.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [2.]\n",
      " [5.]\n",
      " [2.]\n",
      " [4.]\n",
      " [6.]\n",
      " [3.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [6.]\n",
      " [4.]\n",
      " [5.]\n",
      " [5.]\n",
      " [5.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [6.]\n",
      " [2.]\n",
      " [3.]\n",
      " [5.]\n",
      " [5.]\n",
      " [5.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [5.]\n",
      " [5.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [6.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [7.]\n",
      " [5.]\n",
      " [8.]\n",
      " [3.]\n",
      " [3.]\n",
      " [5.]\n",
      " [2.]\n",
      " [6.]\n",
      " [3.]\n",
      " [5.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [6.]\n",
      " [5.]\n",
      " [6.]\n",
      " [5.]\n",
      " [4.]\n",
      " [6.]\n",
      " [4.]\n",
      " [5.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [5.]\n",
      " [4.]\n",
      " [7.]\n",
      " [5.]\n",
      " [3.]\n",
      " [5.]\n",
      " [4.]\n",
      " [2.]\n",
      " [5.]\n",
      " [6.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [7.]\n",
      " [5.]\n",
      " [4.]\n",
      " [5.]\n",
      " [6.]\n",
      " [4.]\n",
      " [3.]\n",
      " [6.]\n",
      " [3.]\n",
      " [3.]\n",
      " [5.]\n",
      " [4.]\n",
      " [2.]\n",
      " [5.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [6.]\n",
      " [3.]\n",
      " [3.]\n",
      " [5.]\n",
      " [4.]\n",
      " [5.]\n",
      " [8.]\n",
      " [2.]\n",
      " [5.]\n",
      " [4.]\n",
      " [6.]\n",
      " [3.]\n",
      " [6.]\n",
      " [6.]\n",
      " [5.]\n",
      " [5.]\n",
      " [5.]\n",
      " [3.]\n",
      " [6.]\n",
      " [0.]\n",
      " [5.]\n",
      " [7.]\n",
      " [4.]\n",
      " [2.]\n",
      " [5.]\n",
      " [3.]\n",
      " [7.]\n",
      " [6.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [5.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [5.]\n",
      " [5.]\n",
      " [2.]\n",
      " [5.]\n",
      " [1.]\n",
      " [2.]\n",
      " [5.]\n",
      " [5.]\n",
      " [4.]\n",
      " [5.]\n",
      " [0.]\n",
      " [5.]\n",
      " [6.]\n",
      " [5.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [5.]\n",
      " [5.]\n",
      " [5.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [8.]\n",
      " [2.]\n",
      " [4.]\n",
      " [6.]\n",
      " [5.]\n",
      " [5.]\n",
      " [6.]\n",
      " [3.]\n",
      " [5.]\n",
      " [7.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [6.]\n",
      " [3.]\n",
      " [5.]\n",
      " [5.]\n",
      " [3.]\n",
      " [6.]\n",
      " [5.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [5.]\n",
      " [0.]\n",
      " [3.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [6.]\n",
      " [6.]\n",
      " [4.]\n",
      " [6.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [6.]\n",
      " [6.]\n",
      " [5.]\n",
      " [2.]\n",
      " [7.]\n",
      " [5.]\n",
      " [3.]\n",
      " [5.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [9.]\n",
      " [6.]\n",
      " [5.]\n",
      " [3.]\n",
      " [5.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [7.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [6.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [6.]\n",
      " [4.]\n",
      " [4.]\n",
      " [6.]\n",
      " [2.]\n",
      " [7.]\n",
      " [4.]\n",
      " [6.]\n",
      " [3.]\n",
      " [4.]\n",
      " [7.]\n",
      " [6.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [6.]\n",
      " [5.]\n",
      " [5.]\n",
      " [1.]\n",
      " [2.]\n",
      " [4.]\n",
      " [6.]\n",
      " [5.]\n",
      " [6.]\n",
      " [3.]\n",
      " [5.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [5.]\n",
      " [5.]\n",
      " [3.]\n",
      " [5.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [5.]\n",
      " [4.]\n",
      " [6.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]\n",
      " [5.]\n",
      " [2.]\n",
      " [4.]\n",
      " [5.]\n",
      " [5.]\n",
      " [3.]\n",
      " [5.]\n",
      " [4.]\n",
      " [3.]\n",
      " [7.]\n",
      " [4.]\n",
      " [6.]\n",
      " [6.]\n",
      " [5.]\n",
      " [5.]\n",
      " [5.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [8.]\n",
      " [2.]\n",
      " [1.]\n",
      " [5.]\n",
      " [2.]\n",
      " [8.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [7.]\n",
      " [5.]\n",
      " [4.]\n",
      " [2.]\n",
      " [7.]\n",
      " [6.]\n",
      " [5.]\n",
      " [6.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [3.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [6.]\n",
      " [2.]\n",
      " [2.]\n",
      " [7.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [5.]\n",
      " [5.]\n",
      " [7.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [5.]\n",
      " [6.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [6.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [6.]\n",
      " [5.]\n",
      " [5.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [7.]\n",
      " [5.]\n",
      " [4.]\n",
      " [5.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [6.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [6.]\n",
      " [2.]\n",
      " [7.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [7.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [5.]\n",
      " [6.]\n",
      " [4.]\n",
      " [7.]\n",
      " [5.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [5.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [5.]\n",
      " [7.]\n",
      " [2.]\n",
      " [4.]\n",
      " [5.]\n",
      " [3.]\n",
      " [5.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [6.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [3.]\n",
      " [4.]\n",
      " [6.]\n",
      " [3.]\n",
      " [5.]\n",
      " [5.]\n",
      " [6.]\n",
      " [5.]\n",
      " [6.]\n",
      " [6.]\n",
      " [5.]\n",
      " [5.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [5.]\n",
      " [5.]\n",
      " [0.]\n",
      " [5.]\n",
      " [6.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [7.]\n",
      " [3.]\n",
      " [6.]\n",
      " [4.]\n",
      " [6.]\n",
      " [3.]\n",
      " [3.]\n",
      " [5.]\n",
      " [2.]\n",
      " [4.]\n",
      " [5.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [7.]\n",
      " [3.]\n",
      " [6.]\n",
      " [6.]\n",
      " [6.]\n",
      " [3.]\n",
      " [7.]\n",
      " [2.]\n",
      " [6.]\n",
      " [5.]\n",
      " [5.]\n",
      " [5.]\n",
      " [5.]\n",
      " [6.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [6.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [5.]\n",
      " [5.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [7.]\n",
      " [4.]\n",
      " [6.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [5.]\n",
      " [3.]\n",
      " [5.]\n",
      " [5.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [1.]\n",
      " [5.]\n",
      " [7.]\n",
      " [4.]\n",
      " [2.]\n",
      " [6.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [5.]\n",
      " [5.]\n",
      " [3.]\n",
      " [6.]\n",
      " [5.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [6.]\n",
      " [5.]\n",
      " [5.]\n",
      " [3.]\n",
      " [5.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [5.]\n",
      " [3.]\n",
      " [4.]\n",
      " [7.]\n",
      " [2.]\n",
      " [4.]\n",
      " [6.]\n",
      " [4.]\n",
      " [1.]\n",
      " [5.]\n",
      " [4.]\n",
      " [3.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [6.]\n",
      " [4.]\n",
      " [3.]\n",
      " [5.]\n",
      " [5.]\n",
      " [5.]\n",
      " [2.]\n",
      " [5.]\n",
      " [2.]\n",
      " [2.]\n",
      " [6.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [6.]\n",
      " [3.]\n",
      " [6.]\n",
      " [6.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [3.]\n",
      " [6.]\n",
      " [5.]\n",
      " [3.]\n",
      " [8.]\n",
      " [6.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [5.]\n",
      " [5.]\n",
      " [4.]\n",
      " [6.]\n",
      " [2.]\n",
      " [3.]\n",
      " [8.]\n",
      " [5.]\n",
      " [5.]\n",
      " [5.]\n",
      " [3.]\n",
      " [5.]\n",
      " [4.]\n",
      " [6.]\n",
      " [2.]\n",
      " [5.]\n",
      " [3.]\n",
      " [4.]\n",
      " [6.]\n",
      " [6.]\n",
      " [5.]\n",
      " [2.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [8.]\n",
      " [2.]\n",
      " [3.]\n",
      " [6.]\n",
      " [5.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [6.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [9.]\n",
      " [4.]\n",
      " [4.]\n",
      " [6.]\n",
      " [2.]\n",
      " [7.]\n",
      " [7.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [5.]\n",
      " [5.]\n",
      " [6.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [7.]\n",
      " [5.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [6.]\n",
      " [4.]\n",
      " [0.]\n",
      " [5.]\n",
      " [6.]\n",
      " [5.]\n",
      " [7.]\n",
      " [5.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [6.]\n",
      " [2.]\n",
      " [6.]\n",
      " [2.]\n",
      " [5.]\n",
      " [5.]\n",
      " [9.]\n",
      " [5.]\n",
      " [6.]\n",
      " [5.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [6.]\n",
      " [5.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [5.]\n",
      " [2.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [6.]\n",
      " [6.]\n",
      " [6.]\n",
      " [5.]\n",
      " [2.]\n",
      " [4.]\n",
      " [5.]\n",
      " [3.]\n",
      " [5.]\n",
      " [2.]\n",
      " [3.]\n",
      " [5.]\n",
      " [4.]\n",
      " [3.]\n",
      " [5.]\n",
      " [5.]\n",
      " [1.]\n",
      " [3.]\n",
      " [4.]\n",
      " [5.]\n",
      " [6.]\n",
      " [5.]\n",
      " [4.]\n",
      " [3.]\n",
      " [5.]\n",
      " [3.]\n",
      " [3.]\n",
      " [5.]\n",
      " [3.]\n",
      " [4.]\n",
      " [6.]\n",
      " [3.]\n",
      " [6.]\n",
      " [5.]\n",
      " [8.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [6.]\n",
      " [6.]\n",
      " [5.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [3.]\n",
      " [5.]\n",
      " [0.]\n",
      " [1.]\n",
      " [2.]\n",
      " [4.]\n",
      " [6.]\n",
      " [2.]\n",
      " [7.]\n",
      " [4.]\n",
      " [5.]\n",
      " [5.]\n",
      " [5.]\n",
      " [2.]\n",
      " [5.]\n",
      " [3.]\n",
      " [5.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [5.]\n",
      " [3.]\n",
      " [4.]\n",
      " [5.]\n",
      " [2.]\n",
      " [2.]\n",
      " [8.]\n",
      " [3.]\n",
      " [6.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [6.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]]\n"
     ]
    }
   ],
   "source": [
    "from math import sin\n",
    "num_sample = 1000\n",
    "num_feature = 1\n",
    "X = 5*np.random.normal(size=(num_sample,num_feature))\n",
    "y = 2*X\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "# encode 参数没看懂\n",
    "# onehot：做哑变量，返回一个稀疏矩阵，每列有几个特征就返回每列特征数*总列数，一列是一个特征中的一个类别。含有该特征的记为1，不含的记为0。\n",
    "# ordinal：返回一个整数，一列是一个特征。\n",
    "# onehot_dense:做哑变量，返回一个密集数组，用的少。\n",
    "dismodel = KBinsDiscretizer(n_bins=10,encode=\"ordinal\",strategy=\"uniform\")\n",
    "KBins = dismodel.fit_transform(X)   \n",
    "# print(KBins)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 线性模型-多项式拟合\n",
    "\n",
    "- 语法：\n",
    "    ```python\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    quadratic_featurizer = PolynomialFeatures(degree=2)\n",
    "    X_train_quadratic = quadratic_featurizer.fit_transform(X_train)\n",
    "    X_test_quadratic = quadratic_featurizer.fit_transform(X_test)\n",
    "    ```\n",
    "- Parameters:  \n",
    "\n",
    "    **degree**：几次  \n",
    "    **random_state**：种子相同，可以复现。  \n",
    "\n",
    "- Attributes:  \n",
    "\n",
    "    **powers_**：各个项分别是几次  \n",
    "    **n_output_features_**：多项式升维后有几个feature  \n",
    "    **n_features_in_**：升维前有几个feature  \n",
    "    **feature_names_in_**：升维前feature名字  \n",
    "\n",
    "- Methods:\n",
    "\n",
    "    **fit(X)**  \n",
    "    **fit_transform(X,y)**\n",
    "    **get_params(X, y, sample_weight)**: 获取参数  \n",
    "    **set_params(para)**: 设置参数 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision Tree\n",
    "[决策树详讲](https://ml.bbbdata.com/site/text/32)\n",
    "- 优点\n",
    "  - 不用标准化，归一化\n",
    "  - 可以同时处理连续变量和离散变量。其他的工具常常只能分析一种变量。\n",
    "  - 运算速度快。训练决策树的成本和数据点的数量为对数关系。\n",
    "  - 利于理解和解释，便于可视化。\n",
    "  - 统计检验可检验模型可靠性。  \n",
    "\n",
    "- 缺点\n",
    "  - 容易过拟合\n",
    "  - 不稳定\n",
    "  - 局部最优\n",
    "  - 每个类别的样本量要平衡（样本数大于特征数）  \n",
    "\n",
    "- 三种算法对比\n",
    "  - 适用范围  \n",
    "    ID3算法只能处理离散特征的分类问题，C4.5能够处理离散特征和连续特征的分类问题，CART算法可以处理离散和连续特征的分类与回归问题。  \n",
    "  - 假设空间：  \n",
    "    ID3和C4.5算法使用的决策树可以是多分叉的，而CART算法的决策树必须是二叉树。\n",
    "  - 优化算法：  \n",
    "    ID3算法没有剪枝策略，当叶子节点上的样本都属于同一个类别或者所有特征都使用过了的情况下决策树停止生长。  \n",
    "    C4.5算法使用预剪枝策略，当分裂后的增益小于给定阈值或者叶子上的样本数量小于某个阈值或者叶子节点数量达到限定值或者树的深度达到限定值，决策树停止生长。  \n",
    "    CART决策树主要使用后剪枝策略。  \n",
    "- 语法\n",
    "  ```python\n",
    "  from sklearn.tree import DecisionTreeRegressor\n",
    "  ```\n",
    "- Parameters（主要看前2/3，最多看前5）  \n",
    "  \n",
    "  **max_leaf_nodes**： 通过限制最大叶子节点数，可以防止过拟合，默认是\"None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制具体的值可以通过交叉验证得到。  \n",
    "\n",
    "  **max_depth**：int or None, optional (default=None) 一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间。常用来解决过拟合。  \n",
    "\n",
    "  **ccp_alpha**：剪枝时的alpha系数，需要剪枝时设置该参数，默认值是不会剪枝的。  \n",
    "\n",
    "  **min_samples_split**：如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分，如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。  \n",
    "\n",
    "  **min_samples_leaf**： 这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝，如果样本量不大，不需要管这个值，大些如10W可以尝试下5  \n",
    "\n",
    "  **random_state**：种子相同，可以复现。  \n",
    "  \n",
    "  **criterion**：（回归树）mse:默认，均方差，mae：平均绝对差，friedman_mse。（分类树）gini或者entropy,前者是基尼系数，后者是信息熵。两种算法差异不大对准确率无影响，信息熵运算效率低一点，因为它有对数运算.一般说使用默认的基尼系数”gini”就可以了，即CART算法。除非你更喜欢类似ID3, C4.5的最优特征选择方法。  \n",
    "\n",
    "  **splitter**：best or random 前者是在所有特征中找最好的切分点，后者是在部分特征中。默认的”best”适合样本量不大的时候，而如果样本数据量非常大，推荐”random” 。  \n",
    "\n",
    "  **max_features**：None（所有），log2，sqrt，N  特征小于50的时候一般使用所有的。  \n",
    "\n",
    "  **min_weight_fraction_leaf**： 这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝默认是0，就是不考虑权重问题。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。    \n",
    "\n",
    "  **class_weight**： 指定样本各类别的的权重，主要是为了防止训练集某些类别的样本过多导致训练的决策树过于偏向这些类别。这里可以自己指定各个样本的权重，如果使用“balanced”，则算法会自己计算权重，样本量少的类别所对应的样本权重会高。（分类树有，回归树无）  \n",
    "\n",
    "  **min_impurity_split**： 这个值限制了决策树的增长，如果某节点的不纯度(基尼系数，信息增益，均方差，绝对差)小于这个阈值则该节点不再生成子节点。即为叶子节点 。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification -- discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "X1 = np.arange(1, 9).reshape(-1, 2)\n",
    "X2 = np.arange(10, 14).reshape(-1, 2)\n",
    "y = np.arange(2, 4).reshape(-1, 1)\n",
    "X1\n",
    "X2\n",
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_binned = poly.fit_transform(X1)\n",
    "X_binned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_binned = poly.transform(X2)\n",
    "line_binned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly.powers_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
