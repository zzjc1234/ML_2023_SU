{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models based on sklearn\n",
    "Referring to  \n",
    "[Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook)  \n",
    "[Official Document](https://scikit-learn.org/stable/user_guide.html)  \n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic sklearn commands "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- multiclass classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression -- continuous\n",
    "\n",
    "- 判断线性or非线性\n",
    "  - 残差图\n",
    "  - 散点图\n",
    "  - 训练多个模型看准确率\n",
    "- 回归指标  \n",
    "  **损失函数常用 MAE、MSE、RMSE**   \n",
    "  **性能评估指标常用R-square**（R-square就是在用均值预测的标准下衡量模型的预测性能）\n",
    "  - 越接近0越好\n",
    "    - SSE (残差平方和)  $$\\sum{|y_i-\\hat{y_i}|^2}$$\n",
    "    - MAE (平均绝对误差)  $$\\sum{\\frac{|y_i-\\hat{y_i}|}{n}}$$\n",
    "    - MSE (均方误差)  $$\\frac{SSE}{n}=\\sum{\\frac{|y_i-\\hat{y_i}|^2}{n}}$$\n",
    "    - RMSE (均方根误差) $$\\sum{\\sqrt{\\frac{|y_i-\\hat{y_i}|^2}{n}}}$$\n",
    "  - 越接近1越好 \n",
    "    - R-squared (确定系数)\n",
    "      - SSR：预测数据与原始均值的平方和\n",
    "      - SST（残差）：原始数据与原始均值的平方和\n",
    "      - SSE：预测数据与原始数据的平方和\n",
    "    - Adjusted R-squared  (调整R方)  \n",
    "\n",
    "    只要增加了更多的变量，R-squared 要么保持不变，要么增加。  \n",
    "    如果增加更多无意义的变量，Adjusted R-squared 会下降；如果加入的特征值是显著的，则adjusted R-squared也会上升。  \n",
    "    在单变量线性回归中，R-squared和adjusted R-squared是一致的。  \n",
    "\n",
    "    **结论**：如果单变量线性回归，则使用 R-squared评估；多变量，则使用adjusted R-squared。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error , r2_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 线性\n",
    "[sklearn线性回归 岭回归 lasso回归示例代码](https://zhuanlan.zhihu.com/p/165493873)\n",
    "- 标准线性回归\n",
    "  - 特征之间相互独立 --> or, 多重共线性\n",
    "- 岭回归\n",
    "  - 可以解决特征数>样本量的问题，有效防止过拟合\n",
    "  - 可以处理高度相关的数据，变量间存在共线性（最小二乘回归得到的系数不稳定，方差很大）\n",
    "- lasso回归  \n",
    "    lasso 容易使得部分权重取 0，所以可以用其做 feature selection，lasso 的名字so即为 selection operator。权重为 0 的 feature 对回归问题没有贡献，直接去掉权重为 0 的 feature，模型的输出值不变。（参考[lasso回归和岭回归](https://www.cnblogs.com/wuliytTaotao/p/10837533.html)）  \n",
    "    lasso 更容易使得权重变为 0，而 ridge 更容易使得权重接近 0。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### 标准线性回归\n",
    "- 语法：\n",
    "    ```python\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    ```\n",
    "- Parameters:  \n",
    "\n",
    "    **fit_intercept**：有无截距  \n",
    "    **copy_X**：True赋值，False覆写  \n",
    "    **normalize**：是否归一化  \n",
    "    **n_jobs**：default=1，使用CPU数，-1为使用所有CPU  \n",
    "\n",
    "- Attributes:  \n",
    "\n",
    "    **coef_**：如果label有两个，即y值有两列，那么是一个2D的array  \n",
    "    **intercept_**：截距  \n",
    "\n",
    "- Methods:\n",
    "\n",
    "    **fit(X, y, sample_weight)**: 训练模型  \n",
    "    **get_params(X, y, sample_weight)**: 获取参数  \n",
    "    **set_params(para)**: 设置参数  \n",
    "    **predict(X)**：预测数据  \n",
    "    **score(X)**：评估，得到R方  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_square_error:1.22\n",
      "R-squared: 0.88\n",
      "coefficient of the model:-2.99\n",
      "intercept of the model:5.03\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# simple linear regression 一元\n",
    "from sklearn.linear_model import LinearRegression as lm\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 定义样本和特征数量\n",
    "num_sample=1000\n",
    "num_feature = 1\n",
    "# num_feature = 2\n",
    "\n",
    "# 生成一些样本和特征\n",
    "ideal_coef = -3\n",
    "# ideal_coef = [2,-3]\n",
    "ideal_intercept = 5\n",
    "feature=np.random.normal(size=(num_sample,num_feature))\n",
    "label=ideal_coef*feature + ideal_intercept + np.random.normal(size=(num_sample,num_feature))\n",
    "# label=ideal_coef[0]*feature[:,0] + ideal_coef[1]*feature[:,1] + ideal_intercept + np.random.normal(size=(num_sample,))\n",
    "\n",
    "# Slice x into training/testing sets\n",
    "X_train = feature[:-100,:]\n",
    "X_test = feature[-100:,:]\n",
    "# Slice y into training/testing sets\n",
    "y_train = label[:-100]\n",
    "y_test = label[-100:]\n",
    "\n",
    "# fit model\n",
    "model = lm()\n",
    "model.fit(X_train,y_train)\n",
    "y_predict=model.predict(X_test)\n",
    "print(\"mean_square_error:%.2f\" %mean_squared_error(y_test,y_predict))\n",
    "print('R-squared: %.2f' %r2_score(y_test, y_predict))\n",
    "# print('R-squared: %.2f' %model.score(X_test, y_test)) 结果同上\n",
    "print(\"coefficient of the model:%.2f\" %model.coef_)\n",
    "print(\"intercept of the model:%.2f\" %model.intercept_)\n",
    "\n",
    "# Plot outputs\n",
    "plt.scatter(X_test, y_test, color='black')\n",
    "plt.plot(X_test, y_predict, color='green', linewidth=3)\n",
    "\n",
    "# 残差\n",
    "for idx, x in enumerate(X_test):\n",
    "    plt.plot([x, x], [y_test[idx], y_predict[idx]], 'g-')\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_square_error:1.15\n",
      "R-squared: 0.92\n",
      "coefficient of the model:2.03, -2.97\n",
      "intercept of the model:5.00\n",
      "Predicted: 8.675786537507499, Target: 9.353807125707482\n",
      "Predicted: 5.342072311532974, Target: 5.002533595340268\n",
      "Predicted: 2.4462482408759865, Target: 5.360985941876249\n",
      "Predicted: 10.257229278272046, Target: 8.998720037652523\n",
      "Predicted: 3.97007491863926, Target: 3.9873203459128668\n",
      "Predicted: 3.78423684795104, Target: 3.962350202660785\n",
      "Predicted: 1.7551473037891303, Target: 1.42469662228188\n",
      "Predicted: -4.574814169463937, Target: -3.071053105786061\n",
      "Predicted: 12.971561982559257, Target: 12.439438644704133\n",
      "Predicted: 4.010643059894808, Target: 4.197408564073417\n",
      "Predicted: 4.54846582731038, Target: 4.695233419003497\n",
      "Predicted: 8.12352803352059, Target: 10.222268134292221\n",
      "Predicted: 4.942169986927049, Target: 5.65835454710348\n",
      "Predicted: 0.6537647273422325, Target: 1.6238751927019903\n",
      "Predicted: 4.0087677141437865, Target: 3.1101726805996766\n",
      "Predicted: 7.694766835507638, Target: 6.754505499005853\n",
      "Predicted: 9.739210323460672, Target: 9.292647613050754\n",
      "Predicted: 8.614566958071809, Target: 9.127099519573074\n",
      "Predicted: 2.1826043010366405, Target: 1.1072669064866463\n",
      "Predicted: 4.112942418416516, Target: 3.912992722905033\n",
      "Predicted: 7.636041030944217, Target: 8.787615922275167\n",
      "Predicted: 2.978553514389168, Target: 3.459261779384282\n",
      "Predicted: 5.4874765354941095, Target: 5.277758485054234\n",
      "Predicted: 12.725294297927768, Target: 12.651090308167593\n",
      "Predicted: 2.364610548728706, Target: 1.675520719928083\n",
      "Predicted: 7.437483460672141, Target: 7.985812166606691\n",
      "Predicted: 3.2042160265729125, Target: 3.917260613430757\n",
      "Predicted: 4.295052298807057, Target: 4.882680525833361\n",
      "Predicted: -2.5625185453003754, Target: -1.5621993311708837\n",
      "Predicted: 1.7978216997522507, Target: -0.4115158397429757\n",
      "Predicted: 1.9002896315935973, Target: 2.4984839261738307\n",
      "Predicted: 1.1476213863113514, Target: 1.4124645343474782\n",
      "Predicted: 10.628467378802062, Target: 11.27031472767915\n",
      "Predicted: 6.3072198524330325, Target: 7.868548448173067\n",
      "Predicted: 7.128191346793573, Target: 7.595080696667197\n",
      "Predicted: 6.049863181107268, Target: 4.1941244971177625\n",
      "Predicted: 5.61130526745154, Target: 7.468977588661603\n",
      "Predicted: 12.248127408889317, Target: 12.466814795480543\n",
      "Predicted: 3.624364967535511, Target: 4.506286133575\n",
      "Predicted: 7.842997808753141, Target: 6.940151960922941\n",
      "Predicted: 3.1953515606905016, Target: 3.0267580162874834\n",
      "Predicted: 1.6340916975712707, Target: 1.0158868607906744\n",
      "Predicted: 9.344366133777687, Target: 9.222190410538504\n",
      "Predicted: 5.8568380807024365, Target: 4.6888464698607635\n",
      "Predicted: 3.683167052456538, Target: 4.685979982112212\n",
      "Predicted: 0.042139809200000045, Target: -2.0642307859763447\n",
      "Predicted: 7.585314630515956, Target: 8.632485669152242\n",
      "Predicted: 1.7496992631738006, Target: 1.9491749747102063\n",
      "Predicted: 1.5344387018917973, Target: 2.7261167072317116\n",
      "Predicted: 1.4476853611736087, Target: 0.7266553536355327\n",
      "Predicted: 8.187145165069436, Target: 7.41951977623396\n",
      "Predicted: 3.476629800635532, Target: 2.526953954601472\n",
      "Predicted: 2.637062897978755, Target: 2.7092050821108487\n",
      "Predicted: 9.207465913070655, Target: 8.738622100514577\n",
      "Predicted: -0.0010962389514581616, Target: -1.2928591677745889\n",
      "Predicted: 6.437232367989031, Target: 7.334226925718701\n",
      "Predicted: 0.753586749389747, Target: -0.03291361914888302\n",
      "Predicted: 8.807749581889974, Target: 10.686501138471694\n",
      "Predicted: 2.8016837193252035, Target: 3.9793602647072883\n",
      "Predicted: 1.6062999736943149, Target: 0.19334298637539105\n",
      "Predicted: 7.197005814916638, Target: 6.881687068392104\n",
      "Predicted: 2.2822333029578044, Target: 2.0900996392192384\n",
      "Predicted: 4.12746690229711, Target: 3.2003033829681184\n",
      "Predicted: 13.87651832058851, Target: 14.273065706385136\n",
      "Predicted: 1.2154531915214282, Target: 2.191835718889875\n",
      "Predicted: 4.971632450966804, Target: 5.2109300103006575\n",
      "Predicted: 7.05657939138689, Target: 8.398392957482166\n",
      "Predicted: 8.620084287191863, Target: 9.223329070838066\n",
      "Predicted: 1.5608097006553479, Target: 1.3266300657370762\n",
      "Predicted: 0.039238922392899056, Target: -0.8089962167562812\n",
      "Predicted: 9.488703678490591, Target: 10.47745462673144\n",
      "Predicted: 6.408679017273089, Target: 7.59532822424686\n",
      "Predicted: 5.729324827241136, Target: 5.600355350628139\n",
      "Predicted: 6.187259815780205, Target: 7.3749533592014\n",
      "Predicted: 5.763455457701065, Target: 6.6525339158643675\n",
      "Predicted: 6.375248430863577, Target: 8.32601738162371\n",
      "Predicted: 5.60154819303389, Target: 5.312043696789004\n",
      "Predicted: 4.886327522233629, Target: 5.773348606340736\n",
      "Predicted: 3.1404770227969534, Target: 2.4970502738882203\n",
      "Predicted: 5.991344420677074, Target: 5.019568087580662\n",
      "Predicted: 2.4985765242908196, Target: 2.7439322729198556\n",
      "Predicted: 1.8603382823604409, Target: 2.974448012942892\n",
      "Predicted: 4.3984919325392555, Target: 1.2963550776925565\n",
      "Predicted: 2.924010724282609, Target: 3.3457424350644898\n",
      "Predicted: 1.7097247209114825, Target: 2.00363660824639\n",
      "Predicted: 0.3975097061070789, Target: -0.36245299883959925\n",
      "Predicted: 5.803301566980159, Target: 6.710294199246389\n",
      "Predicted: 8.657920806455493, Target: 8.19892291113557\n",
      "Predicted: 3.0046028305833996, Target: 2.70798147327846\n",
      "Predicted: 7.2664640680044545, Target: 4.335177127657422\n",
      "Predicted: 1.9231105581708419, Target: 1.7585722323407185\n",
      "Predicted: 10.554147285287414, Target: 11.71507855777793\n",
      "Predicted: 3.374410706282591, Target: 3.3969346949794703\n",
      "Predicted: 8.127745190680136, Target: 7.138111610635091\n",
      "Predicted: 3.577070257627542, Target: 2.9403026295930514\n",
      "Predicted: 3.205597508327317, Target: 4.664034187330346\n",
      "Predicted: -4.243935624892164, Target: -6.1226731905273475\n",
      "Predicted: 4.041947439885897, Target: 3.7786173766173623\n",
      "Predicted: 8.75444284139694, Target: 7.498911951343874\n",
      "Predicted: 5.06776042780776, Target: 6.532920933055202\n"
     ]
    }
   ],
   "source": [
    "# simple linear regression  二元\n",
    "# !!!!! 注意多元线性回归，feature.reshape(1000,)，一定要统一为二维\n",
    "from sklearn.linear_model import LinearRegression as lm\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "import numpy as np\n",
    "\n",
    "# 定义样本和特征数量\n",
    "num_sample=1000\n",
    "# num_feature = 1\n",
    "num_feature = 2\n",
    "\n",
    "# 生成一些样本和特征\n",
    "# ideal_coef = -3\n",
    "ideal_coef = [2,-3]\n",
    "ideal_intercept = 5\n",
    "feature=np.random.normal(size=(num_sample,num_feature))\n",
    "# label=ideal_coef*feature + ideal_intercept + np.random.normal(size=(num_sample,num_feature))\n",
    "label=ideal_coef[0]*feature[:,0] + ideal_coef[1]*feature[:,1] + ideal_intercept + np.random.normal(size=(num_sample,))\n",
    "\n",
    "# Slice x into training/testing sets\n",
    "X_train = feature[:-100,:]\n",
    "X_test = feature[-100:,:]\n",
    "# Slice y into training/testing sets\n",
    "y_train = label[:-100]\n",
    "y_test = label[-100:]\n",
    "\n",
    "# fit model\n",
    "model = lm()\n",
    "model.fit(X_train,y_train)\n",
    "y_predict=model.predict(X_test)\n",
    "print(\"mean_square_error:%.2f\" %mean_squared_error(y_test,y_predict))\n",
    "print('R-squared: %.2f' %r2_score(y_test, y_predict))\n",
    "# print('R-squared: %.2f' %model.score(X_test, y_test)) 结果同上\n",
    "# print(\"coefficient of the model:%.2f\" %model.coef_)\n",
    "print(\"coefficient of the model:%.2f, %.2f\" %(model.coef_[0],model.coef_[1]))\n",
    "print(\"intercept of the model:%.2f\" %model.intercept_)\n",
    "for i, prediction in enumerate(y_predict):\n",
    "    print('Predicted: %s, Target: %s' % (prediction, y_test[i]))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ridge Regression & Lasso Regression\n",
    "- 语法：\n",
    "    ```python\n",
    "    from sklearn.linear_model import Ridge\n",
    "    from sklearn.linear_model import Lasso\n",
    "    from sklearn.linear_model import RidgeCV\n",
    "    from sklearn.linear_model import LassoCV\n",
    "    ```\n",
    "- Parameters:  \n",
    "\n",
    "    **alpha/alphas(CV)**：正则项系数，初始值为1，数值越大，则对复杂模型的惩罚力度越大。 \n",
    "    **cv**： cross-validation generator，默认留一。  \n",
    "    **scoring**：判定best_score的指标。\n",
    "    **random_state**：种子相同，可以复现。\n",
    "    **max_iter**：最大迭代次数。  \n",
    "    **fit_intercept**：有无截距  \n",
    "    **copy_X**：True赋值，False覆写  \n",
    "    **normalize**：是否归一化  \n",
    "    **n_jobs**：default=1，使用CPU数，-1为使用所有CPU  \n",
    "\n",
    "    更多参数见 [sklearn线性回归参数](https://blog.csdn.net/VariableX/article/details/107166602)\n",
    "\n",
    "- 调参方法：  \n",
    "\n",
    "  1. 给定alpha较小的值，例如0.1。\n",
    "  2. 根据验证集准确率以10倍为单位增大或者减小参数值。[0.001, 0.01, 0.1, 1, 10, 100]\n",
    "  3. 在找到合适的数量级后，在此数量级上微调。  \n",
    "    <br>\n",
    "- Attributes (CV)  \n",
    "    **alpha_**  \n",
    "    **best_score_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_square_error:0.72\n",
      "R-squared: 0.94\n",
      "coefficient of the model:2.02, -3.03\n",
      "intercept of the model:4.98\n",
      "best alpha:0.01\n",
      "best score:0.92\n",
      "Predicted: 9.11871464058216, Target: 8.38668977917173\n",
      "Predicted: 2.6178628510108473, Target: 2.536777427531296\n",
      "Predicted: -2.4474458194635833, Target: -2.848522907503656\n",
      "Predicted: -1.8013279706140901, Target: -1.1602773705200902\n",
      "Predicted: -3.3112337194173893, Target: -3.396766679125024\n",
      "Predicted: 1.543199254087507, Target: 1.7473208843016472\n",
      "Predicted: 3.9762939260507455, Target: 3.7317481168063047\n",
      "Predicted: 2.7221720058968684, Target: 4.024719018488776\n",
      "Predicted: 7.524962021807304, Target: 7.673660257263186\n",
      "Predicted: 5.631163139751192, Target: 3.857272129543964\n",
      "Predicted: 11.544032206495068, Target: 12.396311585491981\n",
      "Predicted: 3.30465290462737, Target: 2.8780464579038996\n",
      "Predicted: 0.6320616265486638, Target: 1.3906447983607046\n",
      "Predicted: 7.848222120901337, Target: 9.267375776594779\n",
      "Predicted: 4.44858486600072, Target: 3.988764861924813\n",
      "Predicted: 6.718331217442224, Target: 8.974084417954117\n",
      "Predicted: 6.186147859500281, Target: 4.996588006385076\n",
      "Predicted: 6.571561295928759, Target: 7.0205484094558255\n",
      "Predicted: 6.424119637222055, Target: 5.71959164708511\n",
      "Predicted: 8.200623521376013, Target: 7.136497720388721\n",
      "Predicted: 4.709076773025375, Target: 3.6030033257160197\n",
      "Predicted: 2.070352968612923, Target: 1.3493669234649257\n",
      "Predicted: 3.1503968113128544, Target: 3.578409077846023\n",
      "Predicted: 5.298427702513199, Target: 5.466114562750542\n",
      "Predicted: 8.763247573136061, Target: 10.56465950001257\n",
      "Predicted: 4.403967637634112, Target: 3.9312625905601557\n",
      "Predicted: 9.2834122707871, Target: 9.207942533004015\n",
      "Predicted: 4.463923603470604, Target: 4.141673751912647\n",
      "Predicted: 4.165705304031677, Target: 5.114944753429211\n",
      "Predicted: 9.68154604716644, Target: 9.112797826427723\n",
      "Predicted: 4.599259329534461, Target: 4.847944321069538\n",
      "Predicted: 7.475562832698973, Target: 6.65079857850072\n",
      "Predicted: 8.81714581319242, Target: 8.849141639936885\n",
      "Predicted: 2.3768943789058947, Target: 2.1780318841981248\n",
      "Predicted: 10.06240647551493, Target: 10.41539366579859\n",
      "Predicted: -0.22506871261118366, Target: -0.8937681844312361\n",
      "Predicted: 8.776713741107393, Target: 9.923109344862363\n",
      "Predicted: 11.853074658838548, Target: 11.952319832637762\n",
      "Predicted: 8.671166456541286, Target: 9.08461441212428\n",
      "Predicted: 0.30150629116535477, Target: 1.3526287060074798\n",
      "Predicted: 6.58393355999688, Target: 6.025641533004229\n",
      "Predicted: 10.432710088657126, Target: 10.295022760643343\n",
      "Predicted: 3.374168678118657, Target: 4.550835377733016\n",
      "Predicted: 5.9321203157624325, Target: 5.95001581989875\n",
      "Predicted: 0.8111116421931328, Target: 3.561846648881302\n",
      "Predicted: 7.071376329538624, Target: 7.037975040795133\n",
      "Predicted: 8.341403634521548, Target: 8.010464926011476\n",
      "Predicted: 7.550583244262248, Target: 7.781621549506225\n",
      "Predicted: 9.846130188484844, Target: 9.645227825035851\n",
      "Predicted: 4.144585994669103, Target: 3.3036216407637995\n",
      "Predicted: 2.6477583242686906, Target: 1.8698157647259923\n",
      "Predicted: 4.077292732088255, Target: 3.4101153465434577\n",
      "Predicted: 8.751045060815603, Target: 9.71915666321078\n",
      "Predicted: 4.933910866861333, Target: 5.865460618792741\n",
      "Predicted: 4.7785197666258, Target: 4.24359620195625\n",
      "Predicted: 7.584900139015309, Target: 8.5793782025288\n",
      "Predicted: 1.5259063445462688, Target: 1.7690007371320844\n",
      "Predicted: 6.696930521242042, Target: 5.312839864354583\n",
      "Predicted: 3.350331323328997, Target: 3.094108702892929\n",
      "Predicted: -0.0879320577438536, Target: -0.979955086780225\n",
      "Predicted: 5.809704432882762, Target: 6.81342757927807\n",
      "Predicted: 7.137820555769606, Target: 6.788670548750416\n",
      "Predicted: 6.477618427132839, Target: 7.550062579777781\n",
      "Predicted: 2.6612841743559787, Target: 2.315902703471731\n",
      "Predicted: 4.093582809347484, Target: 4.220696682923766\n",
      "Predicted: 1.1033600792117033, Target: 0.7992291699034135\n",
      "Predicted: -5.261868435685875, Target: -5.298296508828768\n",
      "Predicted: 3.7203231053217127, Target: 5.830796170398068\n",
      "Predicted: 5.714776275232582, Target: 6.304887659385475\n",
      "Predicted: 7.715666362770056, Target: 7.576873622418865\n",
      "Predicted: 7.534445645426734, Target: 8.68216873288045\n",
      "Predicted: 7.410384972864541, Target: 7.790203255680152\n",
      "Predicted: 4.881062358870789, Target: 4.393889989905575\n",
      "Predicted: 6.617174252845029, Target: 6.6766375820696275\n",
      "Predicted: 7.152223612686662, Target: 7.130320503389753\n",
      "Predicted: 3.5259025659131145, Target: 3.2058256660836317\n",
      "Predicted: 4.264644502015011, Target: 2.685835516597935\n",
      "Predicted: 7.191219855920277, Target: 7.042678088989231\n",
      "Predicted: 6.189593443760545, Target: 5.116349911241212\n",
      "Predicted: 6.286919410688628, Target: 6.227564631870244\n",
      "Predicted: 3.71341306815587, Target: 4.425158360523692\n",
      "Predicted: 11.775224944597015, Target: 11.897979746218818\n",
      "Predicted: -0.4142998336163064, Target: 0.20904375845326162\n",
      "Predicted: 4.152039334851537, Target: 3.2156180782943142\n",
      "Predicted: 3.1021124139617675, Target: 4.122807128141245\n",
      "Predicted: 5.322053956985261, Target: 4.687012088488396\n",
      "Predicted: 2.1838987848013147, Target: 3.0528557536990295\n",
      "Predicted: 3.3596383864449075, Target: 3.1623795001417325\n",
      "Predicted: 4.699467792048449, Target: 4.637612545326199\n",
      "Predicted: 5.809474755573662, Target: 6.31968368385995\n",
      "Predicted: 2.0041440365841003, Target: 3.030782469386873\n",
      "Predicted: 5.771978965246231, Target: 4.270043057238636\n",
      "Predicted: 4.5659465930505725, Target: 4.81846742023612\n",
      "Predicted: 10.222957831327573, Target: 10.953039748803885\n",
      "Predicted: 4.068529333189959, Target: 5.350227829721463\n",
      "Predicted: 7.296474551559351, Target: 7.127149400189329\n",
      "Predicted: 6.180835662073125, Target: 6.358720701440326\n",
      "Predicted: 4.957784628079245, Target: 5.023348364676721\n",
      "Predicted: 7.4951910524488685, Target: 8.406873451475125\n",
      "Predicted: 8.600506414552733, Target: 10.274658524098044\n"
     ]
    }
   ],
   "source": [
    "# RidgeCV\n",
    "from sklearn.linear_model import RidgeCV as lm\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "import numpy as np\n",
    "\n",
    "# 定义样本和特征数量\n",
    "num_sample=1000\n",
    "# num_feature = 1\n",
    "num_feature = 2\n",
    "\n",
    "# 生成一些样本和特征\n",
    "# ideal_coef = -3\n",
    "ideal_coef = [2,-3]\n",
    "ideal_intercept = 5\n",
    "feature=np.random.normal(size=(num_sample,num_feature))\n",
    "# label=ideal_coef*feature + ideal_intercept + np.random.normal(size=(num_sample,num_feature))\n",
    "label=ideal_coef[0]*feature[:,0] + ideal_coef[1]*feature[:,1] + ideal_intercept + np.random.normal(size=(num_sample,))\n",
    "\n",
    "# Slice x into training/testing sets\n",
    "X_train = feature[:-100,:]\n",
    "X_test = feature[-100:,:]\n",
    "# Slice y into training/testing sets\n",
    "y_train = label[:-100]\n",
    "y_test = label[-100:]\n",
    "\n",
    "# fit model\n",
    "model = lm(alphas = [0.01,0.1,1,10,100], scoring = 'r2')\n",
    "model.fit(X_train,y_train)\n",
    "y_predict=model.predict(X_test)\n",
    "print(\"mean_square_error:%.2f\" %mean_squared_error(y_test,y_predict))\n",
    "print('R-squared: %.2f' %r2_score(y_test, y_predict))\n",
    "# print('R-squared: %.2f' %model.score(X_test, y_test)) 结果同上\n",
    "# print(\"coefficient of the model:%.2f\" %model.coef_)\n",
    "print(\"coefficient of the model:%.2f, %.2f\" %(model.coef_[0],model.coef_[1]))\n",
    "print(\"intercept of the model:%.2f\" %model.intercept_)\n",
    "print(\"best alpha:%.2f\" %model.alpha_)\n",
    "print(\"best score:%.2f\" %model.best_score_)\n",
    "for i, prediction in enumerate(y_predict):\n",
    "    print('Predicted: %s, Target: %s' % (prediction, y_test[i]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Elastic Net 弹性网\n",
    "##### Perceptron 感知机"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 非线性\n",
    "\n",
    "- 线性模型解决非线性问题\n",
    "  - 分箱\n",
    "  - 多项式回归\n",
    "- Decision tree\n",
    "- Random forest\n",
    "- SVM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 线性模型-分箱\n",
    "\n",
    "- 语法：\n",
    "    ```python\n",
    "    from sklearn.preprocessing import KBinsDiscretizer\n",
    "    ```\n",
    "- Parameters:  \n",
    "\n",
    "    **n_bins**：几个箱子  \n",
    "    **strategy**：{‘uniform’, ‘quantile’, ‘kmeans’}，default=’quantile’，宽度一样，点数一样，kmeans聚类\n",
    "    **random_state**：种子相同，可以复现。  \n",
    "\n",
    "- Attributes:  \n",
    "\n",
    "    **bin_edges_**：边界array  \n",
    "    **n_bins_**：个数int array  \n",
    "    **n_features_in_**  \n",
    "    **feature_names_in_**  \n",
    "\n",
    "- Methods:\n",
    "\n",
    "    **fit(X)**  \n",
    "    **transform(X)**  \n",
    "    **inverse_transform(Xt)**  \n",
    "    **fit_transform(X, y)**  \n",
    "    **get_params(X, y, sample_weight)**: 获取参数  \n",
    "    **set_params(para)**: 设置参数    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_discretization.py:239: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from math import sin\n",
    "num_sample = 1000\n",
    "num_feature = 1\n",
    "X = 5*np.random.normal(size=(num_sample,num_feature))\n",
    "y = 2*X\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "# encode 参数没看懂\n",
    "# onehot：做哑变量，返回一个稀疏矩阵，每列有几个特征就返回每列特征数*总列数，一列是一个特征中的一个类别。含有该特征的记为1，不含的记为0。\n",
    "# ordinal：返回一个整数，一列是一个特征。\n",
    "# onehot_dense:做哑变量，返回一个密集数组，用的少。\n",
    "dismodel = KBinsDiscretizer(n_bins=10,encode=\"ordinal\",strategy=\"uniform\")\n",
    "KBins = dismodel.fit_transform(X)   \n",
    "# print(KBins)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 线性模型-多项式拟合\n",
    "\n",
    "- 语法：\n",
    "    ```python\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    quadratic_featurizer = PolynomialFeatures(degree=2)\n",
    "    X_train_quadratic = quadratic_featurizer.fit_transform(X_train)\n",
    "    X_test_quadratic = quadratic_featurizer.fit_transform(X_test)\n",
    "    ```\n",
    "- Parameters:  \n",
    "\n",
    "    **degree**：几次  \n",
    "    **random_state**：种子相同，可以复现。  \n",
    "\n",
    "- Attributes:  \n",
    "\n",
    "    **powers_**：各个项分别是几次  \n",
    "    **n_output_features_**：多项式升维后有几个feature  \n",
    "    **n_features_in_**：升维前有几个feature  \n",
    "    **feature_names_in_**：升维前feature名字  \n",
    "\n",
    "- Methods:\n",
    "\n",
    "    **fit(X)**  \n",
    "    **fit_transform(X,y)**\n",
    "    **get_params(X, y, sample_weight)**: 获取参数  \n",
    "    **set_params(para)**: 设置参数 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision Tree\n",
    "[决策树详讲](https://ml.bbbdata.com/site/text/32)\n",
    "- 优点\n",
    "  - 不用标准化，归一化\n",
    "  - 可以同时处理连续变量和离散变量。其他的工具常常只能分析一种变量。\n",
    "  - 运算速度快。训练决策树的成本和数据点的数量为对数关系。\n",
    "  - 利于理解和解释，便于可视化。\n",
    "  - 统计检验可检验模型可靠性。  \n",
    "\n",
    "- 缺点\n",
    "  - 容易过拟合\n",
    "  - 不稳定\n",
    "  - 局部最优\n",
    "  - 每个类别的样本量要平衡（样本数大于特征数）  \n",
    "\n",
    "- 三种算法对比\n",
    "  - 适用范围  \n",
    "    ID3算法只能处理离散特征的分类问题，C4.5能够处理离散特征和连续特征的分类问题，CART算法可以处理离散和连续特征的分类与回归问题。  \n",
    "  - 假设空间：  \n",
    "    ID3和C4.5算法使用的决策树可以是多分叉的，而CART算法的决策树必须是二叉树。\n",
    "  - 优化算法：  \n",
    "    ID3算法没有剪枝策略，当叶子节点上的样本都属于同一个类别或者所有特征都使用过了的情况下决策树停止生长。  \n",
    "    C4.5算法使用预剪枝策略，当分裂后的增益小于给定阈值或者叶子上的样本数量小于某个阈值或者叶子节点数量达到限定值或者树的深度达到限定值，决策树停止生长。  \n",
    "    CART决策树主要使用后剪枝策略。  \n",
    "- 语法\n",
    "  ```python\n",
    "  from sklearn.tree import DecisionTreeRegressor\n",
    "  ```\n",
    "- Parameters（主要看前2/3，最多看前5）  \n",
    "  \n",
    "  **max_leaf_nodes**： 通过限制最大叶子节点数，可以防止过拟合，默认是\"None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制具体的值可以通过交叉验证得到。  \n",
    "\n",
    "  **max_depth**：int or None, optional (default=None) 一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间。常用来解决过拟合。  \n",
    "\n",
    "  **ccp_alpha**：剪枝时的alpha系数，需要剪枝时设置该参数，默认值是不会剪枝的。  \n",
    "\n",
    "  **min_samples_split**：如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分，如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。  \n",
    "\n",
    "  **min_samples_leaf**： 这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝，如果样本量不大，不需要管这个值，大些如10W可以尝试下5  \n",
    "\n",
    "  **random_state**：种子相同，可以复现。  \n",
    "  \n",
    "  **criterion**：（回归树）mse:默认，均方差，mae：平均绝对差，friedman_mse。（分类树）gini或者entropy,前者是基尼系数，后者是信息熵。两种算法差异不大对准确率无影响，信息熵运算效率低一点，因为它有对数运算.一般说使用默认的基尼系数”gini”就可以了，即CART算法。除非你更喜欢类似ID3, C4.5的最优特征选择方法。  \n",
    "\n",
    "  **splitter**：best or random 前者是在所有特征中找最好的切分点，后者是在部分特征中。默认的”best”适合样本量不大的时候，而如果样本数据量非常大，推荐”random” 。  \n",
    "\n",
    "  **max_features**：None（所有），log2，sqrt，N  特征小于50的时候一般使用所有的。  \n",
    "\n",
    "  **min_weight_fraction_leaf**： 这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝默认是0，就是不考虑权重问题。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。    \n",
    "\n",
    "  **class_weight**： 指定样本各类别的的权重，主要是为了防止训练集某些类别的样本过多导致训练的决策树过于偏向这些类别。这里可以自己指定各个样本的权重，如果使用“balanced”，则算法会自己计算权重，样本量少的类别所对应的样本权重会高。（分类树有，回归树无）  \n",
    "\n",
    "  **min_impurity_split**： 这个值限制了决策树的增长，如果某节点的不纯度(基尼系数，信息增益，均方差，绝对差)小于这个阈值则该节点不再生成子节点。即为叶子节点 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0, 1, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Sample Code for Decision Tree\n",
    "# Just a demo, plz don't write in that way\n",
    "import numpy as np\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "    \n",
    "    def fit(self, X, y, depth=0):\n",
    "        # if only one class, mark as leaf node\n",
    "        if len(set(y)) == 1:\n",
    "            return {\"leaf\": True, \"class\": y[0]}\n",
    "        \n",
    "        # if depth is larger or equal to the max depth set before, return as a leaf node\n",
    "        # the class of node is the class appears most in the node\n",
    "        if self.max_depth is not None and depth >= self.max_depth:\n",
    "            return {\"leaf\": True, \"class\": max(set(y), key=y.count)}\n",
    "        \n",
    "        best_split = self.find_best_split(X, y)\n",
    "\n",
    "        # if can't find a best split, return as a leaf node\n",
    "        if best_split is None:\n",
    "            return {\"leaf\": True, \"class\": max(set(y), key=y.count)}\n",
    "        \n",
    "        feature, threshold = best_split\n",
    "        left_indices = [i for i, val in enumerate(X[:, feature]) if val <= threshold]\n",
    "        right_indices = [i for i, val in enumerate(X[:, feature]) if val > threshold]\n",
    "        \n",
    "        left_subtree = self.fit(X[left_indices], y[left_indices], depth + 1)\n",
    "        right_subtree = self.fit(X[right_indices], y[right_indices], depth + 1)\n",
    "        \n",
    "        return {\"leaf\": False, \"feature\": feature, \"threshold\": threshold,\n",
    "                \"left\": left_subtree, \"right\": right_subtree}\n",
    "    \n",
    "    def find_best_split(self, X, y):\n",
    "        best_split = None\n",
    "        best_score = float('inf') # smaller the best score, better the best score\n",
    "        for feature in range(X.shape[1]):\n",
    "            thresholds = sorted(set(X[:, feature]))\n",
    "            #use set function to generate a set (mathmatically) and sort the value\n",
    "            for threshold in thresholds:\n",
    "                left_indices = [i for i, val in enumerate(X[:, feature]) if val <= threshold]\n",
    "                right_indices = [i for i, val in enumerate(X[:, feature]) if val > threshold]\n",
    "                \n",
    "                left_labels = y[left_indices]\n",
    "                right_labels = y[right_indices]\n",
    "                \n",
    "                score = len(left_labels) * self.gini_impurity(left_labels) + \\\n",
    "                        len(right_labels) * self.gini_impurity(right_labels)\n",
    "                \n",
    "                if score < best_score:\n",
    "                    best_score = score\n",
    "                    best_split = (feature, threshold)\n",
    "        \n",
    "        return best_split\n",
    "    \n",
    "    def gini_impurity(self, labels):\n",
    "        impurity = 1.0\n",
    "        unique_labels = set(labels)\n",
    "        for label in unique_labels:\n",
    "            p = np.count_nonzero(labels == label) / len(labels)\n",
    "            impurity -= p ** 2\n",
    "        return impurity\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for i in range(X.shape[0]):\n",
    "            predictions.append(self.traverse_tree(X[i], self.tree))\n",
    "        return predictions\n",
    "    \n",
    "    def traverse_tree(self, example, node):\n",
    "        if node[\"leaf\"]:\n",
    "            return node[\"class\"]\n",
    "        \n",
    "        feature = node[\"feature\"]\n",
    "        threshold = node[\"threshold\"]\n",
    "        \n",
    "        if example[feature] <= threshold:\n",
    "            return self.traverse_tree(example, node[\"left\"])\n",
    "        else:\n",
    "            return self.traverse_tree(example, node[\"right\"])\n",
    "\n",
    "X = np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9],\n",
    "    [10, 11, 12],\n",
    "    [13, 14, 15]\n",
    "])\n",
    "\n",
    "y = np.array([0, 1, 0, 1, 1])\n",
    "\n",
    "tree = DecisionTree(max_depth=3)\n",
    "tree.tree = tree.fit(X, y)\n",
    "\n",
    "predictions = tree.predict(X)\n",
    "print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple illustration of decision tree\n",
    "\n",
    "```bash\n",
    "                     feature_1 <= 3.5\n",
    "                     /             \\\n",
    "        class: 0                     feature_2 <= 2.0\n",
    "                                     /             \\\n",
    "                          class: 1                  class: 0\n",
    "                                 \\                  \n",
    "                                  feature_3 <= 5.0\n",
    "                                  /             \\\n",
    "                          class: 1             class: 0\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification -- discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10, 11],\n",
       "       [12, 13]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "X1 = np.arange(1, 9).reshape(-1, 2)\n",
    "X2 = np.arange(10, 14).reshape(-1, 2)\n",
    "y = np.arange(2, 4).reshape(-1, 1)\n",
    "X1\n",
    "X2\n",
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  2.,  1.,  2.,  4.],\n",
       "       [ 1.,  3.,  4.,  9., 12., 16.],\n",
       "       [ 1.,  5.,  6., 25., 30., 36.],\n",
       "       [ 1.,  7.,  8., 49., 56., 64.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_binned = poly.fit_transform(X1)\n",
    "X_binned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.,  10.,  11., 100., 110., 121.],\n",
       "       [  1.,  12.,  13., 144., 156., 169.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_binned = poly.transform(X2)\n",
    "line_binned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [2, 0],\n",
       "       [1, 1],\n",
       "       [0, 2]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly.powers_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
